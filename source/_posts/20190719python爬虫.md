---
layout: post
title: "Python爬虫"
date: 2019-07-19 10:36
toc: true
comments: true
categories: 技术学习
tags: 
	- python
---

#### 爬虫基础

爬虫是模拟浏览器发送请求，获取响应

![image-20200330103508473](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201751.png)

<!--more-->

都说现在是"大数据时代"，那数据从何而来？

- `企业产生的用户数据`：[百度指数](http://index.baidu.com)、[阿里指数](https://alizs.taobao.com)、[TBI腾讯浏览指数](http://tbi.tencent.com)、[新浪微博指数](http://data.weibo.com/index)
- `数据平台购买数据`：[数据堂](http://www.datatang.com/about/about-us.html)、[国云数据市场](http://www.moojnn.com/data-market/)、[贵阳大数据交易所](http://trade.gbdex.com/trade.web/index.jsp)
- `政府/机构公开的数据`：[中华人民共和国国家统计局数据](http://data.stats.gov.cn/index.htm)、[世界银行公开数据](http://data.worldbank.org.cn)、[联合国数据](http://data.un.org)、[纳斯达克](http://www.nasdaq.com/zh)。
- `数据管理咨询公司`：[麦肯锡](http://www.mckinsey.com.cn)、[埃森哲](https://www.accenture.com/cn-zh/)、[艾瑞咨询](http://www.iresearch.com.cn)
- `爬取网络数据`：如果需要的数据市场上没有，或者不愿意购买，那么可以选择招/做一名爬虫工程师

##### 复习导图

![day01复习](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201752.png)

![day02复习](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201753.png)

![day03复习](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201754.png)

![day04复习](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201755.png)![day05总结](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201756.png)

##### 爬虫实际会遇到的问题

**页面上的数据在哪里**

```
当前url地址的elements的内容和url的响应不一样，此时页面上的数据在哪里

- 当前url地址对应的响应中
- 其他的url地址对应的响应中
  - 比如ajax请求中
- js生成的
  - 部分数据在响应中
  - 全部通过js生成
```

**寻找登录的post地址**

```
- 在form表单中寻找action对应的url地址
  - post的数据是input标签中name的值作为键，真正的用户名密码作为值的字典，post的url地址就是action对应的url地址

- 抓包，寻找登录的url地址
  - 勾选perserve log按钮，防止页面跳转找不到url
  - 寻找post数据，确定参数
    - 参数不会变，直接用，比如密码不是动态加密的时候
    - 参数会变
      - 参数在当前的响应中
      - 通过js生成
```

**定位想要的**js

```
- 选择会触发js时间的按钮，点击event listener，找到js的位置
- 通过chrome中的search all file来搜索url中关键字
- 添加断点的方式来查看js的操作，通过python来进行同样的操作
```

##### HTTP 基础知识

```python
HTTP 超文本传输协议  默认端口号:80
HTTPS HTTP + SSL(安全套接字层) 默认端口号：443
HTTPS比HTTP更安全，但是性能更低

url(Uniform / Universal Resource Locator)形式
    基本格式：scheme://host[:port#]/path/…/[?query-string][#anchor]
    scheme：协议(例如：http, https, ftp)
    host：服务器的IP地址或者域名
    port#：服务器的端口（如果是走协议默认端口，缺省端口80 or 443）
    path：访问资源的路径
    query-string：参数，发送给http服务器的数据
    anchor：锚（跳转到网页的指定锚点位置）
    例如：
        ftp://192.168.0.116:8080/index
        http://www.baidu.com
        http://item.jd.com/11936238.html#product-detail

HTTP请求方法
    • GET 向指定的资源发出“显示”请求。使用 GET 方法应该只用在读取数据
    • HEAD 与 GET 方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）
    • POST 向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。
    • PUT 向指定资源位置上传其最新内容。
    • DELETE 请求服务器删除 Request-URI 所标识的资源。
    • TRACE 回显服务器收到的请求，主要用于测试或诊断。
                       
HTTP请求示例                       
    GET https://www.baidu.com/ HTTP/1.1 #请求行: 请求方法 URL 协议版本 回车换行
    Host: www.baidu.com #Host (主机和端口号)
    Connection: keep-alive # keep-alive能够重用连接，减少资源消耗，缩短响应时间
    Upgrade-Insecure-Requests: 1 #升级为HTTPS请求
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36 #浏览器名称
    Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
      	# q是权重系数，q值越大，请求越倾向于获得其类型表示内容
        # Text：文本信息，可以是多种字符集或多种格式；Application：传输应用程序数据或者二进制数据
    Referer: http://www.baidu.com/ #产生请求网页来自于哪个URL
    Accept-Encoding: gzip, deflate, sdch, br #浏览器可接受的编码方式
    Accept-Language: zh-CN,zh;q=0.8,en;q=0.6
    x-requested-with :XMLHttpRequest  # Ajax 异步请求
    Cookie: BAIDUID=04E4001F34EA74AD4601512DD3C41A7B:FG=1; BIDUPSID=04E4001F34EA74AD4601512DD3C41A7B; PSTM=1470329258; ........

```

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201757.png)

HTTP响应也由四个部分组成，分别是： `状态行`、`消息报头`、`空行`、`响应正文`

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201758.jpg)

```python
HTTP响应示例
    HTTP/1.1 200 OK
    Server: Tengine #服务器的信息
    Connection: keep-alive #客户端可以继续使用这个tcp连接发送http请求
    Date: Wed, 30 Nov 2016 07:58:21 GMT #服务端发送资源时的服务器时间
    Cache-Control: no-cache #不会缓存服务器资源
    Content-Type: text/html;charset=UTF-8 #资源文件的类型，还有字符编码
    Content-Encoding:gzip #服务端发送资源采用gzip编码
    Keep-Alive: timeout=20 
    Vary: Accept-Encoding #缓存压缩文件和非压缩文件两个版本
    Pragma: no-cache #与Cache-Control等同
    X-NWS-LOG-UUID: bd27210a-24e5-4740-8f6c-25dbafa9c395
    Content-Length: 180945

    <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" ....
```

```python
常见状态码：
100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。
200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。
300~399：为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）。
400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。
500~599：服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）。
```

##### 请求的python代码

访问 https://curl.trillworks.com/

![screenshot of chrome devtools](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201759.png)

##### robots协议

禁止所有机器人访问特定目录

```python
User-agent: *
Disallow: / * .php$
Disallow: /images/
```

• 人工阅读 robots.txt
• 使用 urllib.robotparser

```python
>>> import urllib.robotparser
>>> rp = urllib.robotparser.RobotFileParser()
>>> rp.set_url("http://127.0.0.1:8000/robots.txt")
>>> rp.read()
>>> rp.can_fetch("Baiduspider", "http://127.0.0.1:8000/")
False
```

##### requests

```python
requests的底层实现就是urllib
requests在python2 和python3中通用，方法完全一样
Requests能够自动帮助我们解压(gzip压缩的等)网页内容

>>> r = requests.post(’http://httpbin.org/post’, data = {’key’:’value’})//post
>>> r = requests.get(’https://api.github.com/user’, auth=(’user’, ’pass’))//get

response的常用方法：
    response.text 
    respones.content
    response.status_code
    response.request.headers
    response.headers

### requests中解决编解码的方法
response.text
    类型：str
    解码类型： 根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码
    如何修改编码方式：response.encoding=”gbk”
    在你调用response.text 方法时，Requests库首先在 HTTP 头部检测是否存在指定的编码方式
    如果不存在，则会使用 chardet.detect来尝试猜测编码方式（存在误差）
    更推荐使用response.content.deocde()
response.content
    类型：bytes
    解码类型： 没有指定
    如何修改编码方式：response.content.deocde(“utf8”)
    
### request技巧
1、reqeusts.util.dict_from_cookiejar  把cookie对象转化为字典
	requests.get(url,cookies={})
2、请求 SSL证书验证
	如果SSL证书验证不通过，或者不信任服务器的安全证书，则会报出SSLError，解决：
        response = requests.get("https://www.12306.cn/mormhweb/ ", verify=False)
3、设置超时
        response = requests.get(url,1)
4、配合状态码判断是否请求成功
       assert response.status_code == 200
```

```python
# coding=utf-8
import requests

class TiebaSpider:
    def __init__(self, tieba_name):
        self.tieba_name = tieba_name
        self.url_temp = "https://tieba.baidu.com/f?kw=" + tieba_name + "&ie=utf-8&pn={}"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"}

    def get_url_list(self):  # 1.构造url列表
        return [self.url_temp.format(i * 50) for i in range(1000)]

    def parse_url(self, url):  # 发送请求，获取响应
        print(url)
        response = requests.get(url, headers=self.headers)
        return response.content.decode()

    def save_html(self, html_str, page_num):  # 保存html字符串
        file_path = "{}—第{}页.html".format(self.tieba_name, page_num)
        with open(file_path, "w", encoding="utf-8") as f:  # "李毅—第4页.html"
            f.write(html_str)

    def run(self):  # 实现主要逻辑
        # 1.构造url列表
        url_list = self.get_url_list()
        # 2.遍历，发送请求，获取响应
        for url in url_list:
            html_str = self.parse_url(url)
            # 3.保存
            page_num = url_list.index(url) + 1  # 页码数
            self.save_html(html_str, page_num)

if __name__ == '__main__':
    tieba_spider = TiebaSpider("lol")
    tieba_spider.run()
```

##### headers 和 查询参数

```python
# 模拟浏览器，欺骗服务器，获取和浏览器一致的内容
import requests
kw = {'wd':'长城'}
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
# params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()
response = requests.get("http://www.baidu.com/s?", params = kw, headers = headers)
# 查看响应内容，response.text 返回的是Unicode格式的数据
print (response.text)
# 查看响应内容，response.content返回的字节流数据，用来保存图片等二进制文件。
print (response.content)
# 查看完整url地址 'http://www.baidu.com/s?wd=%E9%95%BF%E5%9F%8E'
print (response.url) 
# 查看响应头部字符编码 'utf-8'
print (response.encoding)
# 查看响应码 200
print (response.status_code)
```

##### 获取网络上图片

```python
#很多时候，数据读写不一定是文件，也可以在内存中读写。
#StringIO顾名思义就是在内存中读写str。
#BytesIO 就是在内存中读写bytes类型的二进制数据
from io import BytesIO,StringIO
import requests
from PIL import Image
img_url = "https://www.lgstatic.com/lg-www-fed/common/widgets/un_login_banner/img/logo_41a2761.png"
response = requests.get(img_url)
f = BytesIO(response.content)
img = Image.open(f)
print(img.size) #(500, 262)
#保存本地
with open("a.png","wb") as f:
    f.write(response.content)
```

##### POST请求（data参数）

```python
# coding=utf-8
# 百度翻译，使用手机版发请求直接解析返回的json
import requests
import json
query_string = "你好"
headers = {
    'authority': 'fanyi.baidu.com',
    'sec-fetch-dest': 'empty',
    'x-requested-with': 'XMLHttpRequest',
    'user-agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1',
    'dnt': '1',
    'content-type': 'application/x-www-form-urlencoded',
    'accept': '*/*',
    'origin': 'https://fanyi.baidu.com',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-mode': 'cors',
    'referer': 'https://fanyi.baidu.com/',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': 'PSTM=1583771189; BAIDUID=82AB26D9946778044363D81EB480758B:FG=1; BIDUPSID=D55F1C368A38A86C0C8D634251D84604; BDUSS=FwR01LbGM5SWtiUUYwdEYwMWZNVGpyUHBrUWVZRkFnTnc2NUpYM3ZYbWlPcGRlRVFBQUFBJCQAAAAAAAAAAAEAAABQ8c1y1Ma1rVdlYrfnx-UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKKtb16irW9eND; BDORZ=FFFB88E999055A3F8A630C64834BD6D0; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1585541063; from_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; to_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; __yjsv5_shitong=1.0_7_964ef489256a9c4161d0c7e88d69396bb315_700_1585541463801_115.239.231.206_c4722203; Hm_lvt_afd111fa62852d1f37001d1f980b6800=1585541762; yjs_js_security_passport=909599cd2d7c0ed00d05bc5dab30cd79750a049e_1585541763_js; Hm_lpvt_afd111fa62852d1f37001d1f980b6800=1585541772; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1585541772',
}
data = {
  'query': query_string,
  'from': 'zh',
  'to': 'en',
  'token': '9440ed16669f345f99b892813315d08c',
  'sign': '232427.485594'
}
post_url = "https://fanyi.baidu.com/basetrans"
r = requests.post(post_url,data=data,headers=headers)
# print(r.content.decode())
dict_ret = json.loads(r.content.decode())
ret = dict_ret["trans"][0]["dst"]
print("result is :",ret)
```

##### 使用代理ip（proxies参数）

```python
让服务器以为不是同一个客户端在请求
防止我们的真实地址被泄露，防止被追究
用法：requests.get("http://www.baidu.com", proxies = proxies)

- 准备一堆的ip地址，组成ip池，随机选择一个ip来时用
- 如何随机选择代理ip，让使用次数较少的ip地址有更大的可能性被用到
  - {"ip":ip,"times":0}
  - [{},{},{},{},{}],对这个ip的列表进行排序，按照使用次数进行排序
  - 选择使用次数较少的10个ip，从中随机选择一个
- 检查ip的可用性
  - 可以使用requests添加超时参数，判断ip地址的质量
  - 在线代理ip质量检测的网站
  
import requests
# 根据协议类型，选择不同的代理
proxies = {
  "http": "http://12.34.56.79:9527",
  "https": "http://12.34.56.79:9527",
}
response = requests.get("http://www.baidu.com", proxies = proxies)
print response.text
```

![image-20200330122726131](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201800.png)

免费的开放代理获取基本没有成本，我们可以在一些代理网站上收集这些免费代理，测试后如果可以用，就把它收集起来用在爬虫上面。免费短期代理网站举例：

- [西刺免费代理IP](http://www.xicidaili.com/)
- [快代理免费代理](http://www.kuaidaili.com/free/inha/)
- [Proxy360代理](http://www.proxy360.cn/default.aspx)
- [全网代理IP](http://www.goubanjia.com/free/index.shtml)

##### cookie/session(cookies参数)

```python
cookie、session区别
	cookie数据存放在客户的浏览器上，session数据放在服务器上。
    cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗。
	session对象是一个非常常用的对象，这个对象代表一次用户会话
    session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能。
    单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。

带上cookie、session的好处：
	能够请求到登录之后的页面
	携带一堆cookie进行请求，把cookie组成cookie池

带上cookie、session的弊端：
	一套cookie和session往往和一个用户对应
	请求太快，请求次数太多，容易被服务器识别为爬虫

不需要cookie的时候尽量不去使用cookie
但是为了获取登录之后的页面，我们必须发送带有cookies的请求

cookies字符串转dic
cookies = {i.split("=")[0]:i.split("=")[1] for i in cookies.split("; ")}
```

##### 获取登录后页面

- 实例化session，使用session发送post请求，在使用他获取登陆后的页面
- headers中添加cookie键，值为cookie字符串
- 在请求方法中添加cookies参数，接收字典形式的cookie。字典形式的cookie中的键是cookie的name对应的值，值是cookie的value对应的值

```python
# coding=utf-8
import requests
# 使用requests提供的session类来请求登陆之后的网站
session = requests.session()
post_url = "http://www.renren.com/PLogin.do"
post_data = {"email":"mr_mao_hacker@163.com", "password":"alarmchime"}
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"
}
#使用session发送post请求，cookie保存在其中
session.post(post_url,data=post_data,headers=headers)
#在使用session进行请求登陆之后才能访问的地址
r = session.get("http://www.renren.com/327550029/profile",headers=headers)
#保存页面
with open("renren1.html","w",encoding="utf-8") as f:
    f.write(r.content.decode())

# 在请求方法中添加cookies参数
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36",
}
#列表推导式处理cookies
cookies="anonymid=j3jxk555-nrn0wh; _r01_=1; _ga=GA1.2.1274811859.1497951251; _de=BF09EE3A28DED52E6B65F6A4705D973F1383380866D39FF5; ln_uact=mr_mao_hacker@163.com; depovince=BJ; jebecookies=54f5d0fd-9299-4bb4-801c-eefa4fd3012b|||||; JSESSIONID=abcI6TfWH4N4t_aWJnvdw; ick_login=4be198ce-1f9c-4eab-971d-48abfda70a50; p=0cbee3304bce1ede82a56e901916d0949; first_login_flag=1; ln_hurl=http://hdn.xnimg.cn/photos/hdn421/20171230/1635/main_JQzq_ae7b0000a8791986.jpg; t=79bdd322e760beae79c0b511b8c92a6b9; societyguester=79bdd322e760beae79c0b511b8c92a6b9; id=327550029; xnsid=2ac9a5d8; loginfrom=syshome; ch_id=10016; wp_fold=0"
cookies = {i.split("=")[0]:i.split("=")[1] for i in cookies.split("; ")}
print(cookies)
r = requests.get("http://www.renren.com/327550029/profile",headers=headers,cookies=cookies)
```

**web客户端验证**

如果是Web客户端验证，需要添加 auth = (账户名, 密码)

```python
import requests
auth=('test', '123456')
response = requests.get('http://192.168.199.107', auth = auth)
print (response.text)
```

##### urllib库

```python
# 向指定的url发送请求，并返回服务器响应的类文件对象
response = urllib.request.urlopen("http://www.baidu.com")
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容，返回字符串
html = response.read()

#IE 9.0 的 User-Agent
header = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
request = urllib.request.Request(url, headers = header)
#也可以通过调用Request.add_header() 添加/修改一个特定的header
request.add_header("Connection", "keep-alive")
# 也可以通过调用Request.get_header()来查看header信息
# request.get_header(header_name="Connection")
response = urllib.request.urlopen(request)
print (response.code) #可以查看响应状态码
html = response.read().decode()
print (html)

# 通过urllib.urlencode()方法，将字典键值对按URL编码转换，从而能被web服务器接受。
In [3]: urllib.parse.urlencode(word)  
Out[3]: "wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2"
# 通过urllib.unquote()方法，把 URL编码字符串，转换回原先字符串。
In [4]: print urllib.parse.unquote("wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2")
wd=传智播客

# 构建一个HTTPHandler 处理器对象，支持处理HTTP请求
http_handler = urllib.request.HTTPHandler()
# 构建一个HTTPHandler 处理器对象，支持处理HTTPS请求
# http_handler = urllib.request.HTTPSHandler()
# 调用urllib.request.build_opener()方法，创建支持处理HTTP请求的opener对象
opener = urllib.request.build_opener(http_handler)
# 构建 Request请求
request = urllib.request.Request("http://www.baidu.com/")
# 调用自定义opener对象的open()方法，发送request请求
response = opener.open(request)
# 获取服务器响应内容
print (response.read().decode())

# 构建了代理Handler
httpproxy_handler = urllib.request.ProxyHandler({"http" : "124.88.67.81:80"})
opener = urllib.request.build_opener(httpproxy_handler)

cookiejar模块：主要作用是提供用于存储cookie的对象
HTTPCookieProcessor处理器：主要作用是处理这些cookie对象，并构建handler对象。
```

#### 数据提取

##### Xpath

```python
lxml是一款高性能Python HTML/XML 解析器，可以利用XPath来快速的定位特定元素以及获取节点信息
XPath (XML Path Language) 是一门在 HTML\XML 文档中查找信息的语言，可用来在 HTML\XML 文档中对元素和属性进行遍历。
使用chrome插件选择标签时候，选中时，选中的标签会添加属性class="xh-highlight"
lxml使用入门：
    导入lxml 的 etree 库 
        from lxml import etree
    利用etree.HTML，将字符串转化为Element对象
    Element对象具有xpath的方法
        html = etree.HTML(text) 
        
- 使用xpath helper或者是chrome中的copy xpath都是从element中提取的数据，但是爬虫获取的是url对应的响应，往往和elements不一样
- 获取文本
  - `a/text()` 获取a下的文本
  - `a//text()` 获取a下的所有标签的文本
  - `//a[text()='下一页']` 选择文本为下一页三个字的a标签

- `@符号`
  - `a/@href`
  - `//ul[@id="detail-list"]`

- `//`
  - 在xpath最前面表示从当前html中任意位置开始选择
  - `li//a` 表示的是li下任何一个标签

- xpath的包含
	- //div[contains(@class,'i')]
```

| 表达式   | 描述                                                  |
| -------- | ----------------------------------------------------- |
| nodename | 选取此节点的所有子节点。                              |
| /        | 从根节点选取。                                        |
| //       | 从匹配选择的当前节点选择文档中的节点,可选匹配孙子节点 |
| .        | 选取当前节点。                                        |
| ..       | 选取当前节点的父节点。                                |
| @        | 选取属性。                                            |

| 表达式          | 路径表达式                                                   |
| --------------- | ------------------------------------------------------------ |
| bookstore       | 选取 bookstore 元素的所有子节点。                            |
| /bookstore      | 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ |
| bookstore/book  | 选取属于 bookstore 的子元素的所有 book 元素。                |
| //book          | 选取所有 book 子元素，而不管它们在文档中的位置。             |
| bookstore//book | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 |
| //@lang         | 选取名为 lang 的所有属性                                     |

![image-20200404181710776](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201801.png)

![image-20200404182026176](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201802.png)

![image-20200404182136384](https://cdn.jsdelivr.net/gh/siyuanzhou/pic/img/20210329201803.png)

```python
# coding=utf-8
import requests
from lxml import etree

class QiubaiSpdier:
    def __init__(self):
        self.url_temp = "https://www.qiushibaike.com/8hr/page/{}/"

        self.headers = {"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"}
    def get_url_list(self):
        return [self.url_temp.format(i) for i in range(1,14)]

    def parse_url(self,url):
        print(url)
        response = requests.get(url,headers=self.headers)
        return response.content.decode()

    def get_content_list(self,html_str): #提取数据
        html = etree.HTML(html_str)
        div_list = html.xpath("//div[@class='recommend-article']//li")  #分组
        content_list = []
        for div in div_list:
            item= {}
            item["content"] = div.xpath(".//div[@class='recmd-right']/a[@class='recmd-content']/text()")
            item["content"] = [i.replace("\n","") for i in item["content"]]
            item["href"] = div.xpath(".//div[@class='recmd-right']/a[@class='recmd-content']/@href")
            item["href"]="https://www.qiushibaike.com"+item["href"][0] if len(item["href"])>0 else None
            item["content_img"] = div.xpath(".//a[contains(@class,'recmd-left')]/img/@src")
            item["content_img"] = "https:"+item["content_img"][0] if len(item["content_img"])>0 else None
            item["author_img"] = div.xpath(".//a[@class='recmd-user']/img/@src")
            item["author_img"] = "https:"+item["author_img"][0] if len(item["author_img"])>0 else None
            item["stats_vote"] = div.xpath(".//div[@class='recmd-num']/span/text()")
            item["stats_vote"] = item["stats_vote"][0] if len(item["stats_vote"])>0 else None
            content_list.append(item)
        return content_list

    def save_content_list(self,content_list): #保存
        for i in content_list:
            print(i)

    def run(self): #实现主要逻辑
        #1.url_list
        url_list = self.get_url_list()
        #2.遍历，发送请求，获取响应
        for url in url_list:
            html_str = self.parse_url(url)
            #3.提取数据
            content_list = self.get_content_list(html_str)
            #4.保存
            self.save_content_list(content_list)

if __name__ == '__main__':
    qiubai = QiubaiSpdier()
    qiubai.run()
```

##### BeautifulSoup

和 lxml 一样，Beautiful Soup 也是一个HTML/XML的解析器，将复杂 HTML 文档转换成一个复杂的树形结构, 每个节点都是 Python 对象

所有对象可以归纳为 4 种: Tag , NavigableString ,BeautifulSoup , Comment 

> lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。

|   抓取工具    | 速度 | 使用难度 |  安装难度  |
| :-----------: | :--: | :------: | :--------: |
|     正则      | 最快 |   困难   | 无（内置） |
| BeautifulSoup |  慢  |  最简单  |    简单    |
|     lxml      |  快  |   简单   |    一般    |

```python
from bs4 import BeautifulSoup
html = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""
#创建 Beautiful Soup 对象
soup = BeautifulSoup(html)
#格式化输出 soup 对象的内容 html文档
print soup.prettify() 

对于 Tag，它有两个重要的属性，是 name 和 attrs
每个 tag 都有自己的名字, 通过.name 来获取:
一个 tag 可能有很多个属性 <b class="boldest"> 有一个“class”的属性, 值为“boldest”
也可以直接”点”取属性, 比如: .attrs eg: soup.p.attrs
用 .string 即可获取标签内部的文字 
tag 的 .content 属性可以将tag的子节点以列表的方式输出
#通过点取属性的方式只能获得当前名字的第一个 tag:
soup.a
# <a class="sister" href="http://example.com/elsie" >
print type(soup.p)
# <class 'bs4.element.Tag'>
print soup.name
# [document] #soup 对象本身比较特殊，它的 name 即为 [document]

# 如果想要得到所有的 <a> 标签, 或是通过名字得到比一个 tag 更多的内容的时候 find_all()
soup.find_all(’a’)
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
# <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
# <a class="sister" href="http://example.com/tillie"id="link3">Lacie</a>,

#也可以直接”点”取属性, 比如: .attrs 。把 p 标签的所有属性打印输出了出来，得到的类型是一个字典
print soup.p.attrs
# {'class': ['title'], 'name': 'dromouse'}
print soup.p['class'] # soup.p.get('class')
# ['title'] #还可以利用get方法，传入属性的名称，二者是等价的
soup.p['class'] = "newClass"
print soup.p # 可以对这些属性和内容等等进行修改
# <p class="newClass" name="dromouse"><b>The Dormouse's story</b></p>
del soup.p['class'] # 还可以对这个属性进行删除
print soup.p
# <p name="dromouse"><b>The Dormouse's story</b></p>

# 既然我们已经得到了标签的内容，要想获取标签内部的文字怎么办呢？很简单，用 .string 即可
print soup.p.string
# The Dormouse's story
如果一个标签里面没有标签了，那么 .string 就会返回标签里面的内容。如果标签里面只有唯一的一个标签了，那么 .string 也会返回最里面的内容。
print soup.head.string
#The Dormouse's story
print soup.title.string
#The Dormouse's story
print type(soup.p.string)
# In [13]: <class 'bs4.element.NavigableString'>
print soup.a.string
# Elsie 
print type(soup.a.string)
# <class 'bs4.element.Comment'>
a 标签里的内容实际上是注释，但是如果我们利用 .string 来输出它的内容时，注释符号已经去掉了。

# tag 的 .content 属性可以将tag的子节点以列表的方式输出
print soup.head.contents 
#[<title>The Dormouse's story</title>]
```

```python
• .descendants 表示子节点
• .next_element 表示下一个节点
• .next_sibling 表示下一个兄弟
• .get_text() 返回文本

通过tag的.children, 它返回的不是一个list，是一个list生成器对象,不过我们可以通过遍历获取所有子节点。
for child in title_tag.children:
	print(child)
	# The Dormouse’s story
	
继续分析文档树, 每个 tag 或字符串都有父节点: 被包含在某个 tag 中通过.parent 属性来获取某个元素的父节点. 
title_tag = soup.title
title_tag
# <title>The Dormouse’s story</title>
title_tag.parent
# <head><title>The Dormouse’s story</title></head>
```

##### CSS选择器

```python
Beautiful Soup 支持大部分的 CSS 选择器, 在 Tag 或 BeautifulSoup 对象的.select() 方法中传入字符串参数, 即可使用 CSS 选择器的语法找到tag:
soup.select("title")
# [<title>The Dormouse’s story</title>]

通过tag标签名逐层查找:
1. 后代选择器
* 语法： ("A B") 选择A元素内部的所有B元素,包括子元素的子元素	
2. 子选择器
* 语法： ("A > B") 选择父元素为A元素的B元素
soup.select("html head title")
# [<title>The Dormouse’s story</title>]

通过 CSS 的类名查找:
soup.select(".sister")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
# <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
# <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

通过 tag 的 id 查找:
soup.select("#link1")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]
soup.select("a#link2")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]

通过是否存在某个属性来查找:
soup.select(’a[href]’)
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
# <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
# <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

通过属性的值来查找:
soup.select(’a[href="http://example.com/elsie"]’)
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]
soup.select(’a[href^="http://example.com/"]’)
soup.select(’a[href * =".com/el"]’)
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

获取内容
以上的 select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容。
soup.select('title')[0].get_text()
```

##### find_all() 选择器

`find_all(name, attrs, recursive, text, **kwargs)`

find_all() 方法搜索当前 tag 的所有 tag 子节点, 并判断是否符合过滤器的条件. 

```python
name 参数可以查找所有名字为 name 的 tag, 字符串对象会被自动忽略掉.传字符串,传正则表达式,传列表
soup.find_all("title")
# [<title>The Dormouse’s story</title>]

# keyword 参数
soup.find_all(id="link2")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]

如果传入 href 参数,Beautiful Soup 会搜索每个 tag 的”href”属性:
soup.find_all(href=re.compile("elsie"))
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

使用多个指定名字的参数可以同时过滤 tag 的多个属性:
soup.find_all(href=re.compile("elsie"), id=’link1’)
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的 tag:
data_soup.find_all(attrs={"data-foo": "value"})
# [<div data-foo="value">foo!</div>]

按照CSS类名搜索tag的功能非常实用, 但class在Python中是保留字, 会导致语法错误.
从Beautiful Soup 的 4.1.1 版本开始, 可以通过 class_ 参数搜索有指定CSS 类名的 tag
soup.find_all("a", class_="sister")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
# <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
# <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

tag 的 class 属性是多值属性. 按照 CSS 类名搜索 tag 时, 可以分别搜索tag 中的每个 CSS 类名:
css_soup = BeautifulSoup(’<p class="body strikeout"></p>’)
css_soup.find_all("p", class_="strikeout")
# [<p class="body strikeout"></p>]

通过 text 参数可以搜搜文档中的字符串内容.
text 参数接受字符串, 正则表达式, 列表, True . 看例子:
soup.find_all(text=["Tillie", "Elsie", "Lacie"])
# [u’Elsie’, u’Lacie’, u’Tillie’]
soup.find_all(text=re.compile("Dormouse"))
[u"The Dormouse’s story", u"The Dormouse’s story"]

搜索内容里面包含“Elsie”的 <a> 标签:
soup.find_all("a", text="Elsie")
#[<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

find_all() 和 find() 只搜索当前节点的所有子节点, 孙子节点等.
find_parents() 和 find_parent() 用来搜索当前节点的父辈节点
a_string = soup.find(text="Lacie") # u’Lacie’
a_string.find_parents("a")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]
```

##### selenium使用

```
1 配置好chromedriver
	打开chrome 输入 “chrome://version/”来查看chrome版本
	访问 http://chromedriver.storage.googleapis.com/index.html 选择合适版本的driver 
	将下载好的驱动放止python的安装目录下即可，anaconda注意要放到对应环境env的目录下
```

##### 爬虫项目案例

给定搜索词集合，爬取谷歌图片和其相关图片

```python
import json
import multiprocessing
import os
import re
import time
from multiprocessing.pool import Pool
import requests
from bs4 import BeautifulSoup

def download_page(url):
    try:
        proxies = {
            'http': 'http://127.0.0.1:1080',
            'https': 'https://127.0.0.1:1080'
        }
        headers = {
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/56.0.2924.87 Safari/537.36',
            'referer': 'https://www.google.com/'
        }
        response = requests.get(url, proxies=proxies, headers=headers, timeout=30)
        return response.text
    except Exception as e:
        print(f'error while downloading page {url}')
        print(e)
        # 返回空串
        return ''

# 解析出各二级标签
def parse_page(first, url, src):
    path = f'{src}/{first}'
    if not os.path.exists(path):
        os.makedirs(path)
    soup = BeautifulSoup(download_page(url), 'lxml')
    second_list = soup.select('div.EDblX.DAVP1 > a')
    for second_a in second_list:
        span = second_a.select('span')
        second = span[0].string.strip()
        href = 'https://www.google.com' + second_a['href']
        while not parse_second(src, first, second, href):
            pass
        time.sleep(2)
    print('All Finised!')

# 解析二级标签详细页
def parse_second(src, first, second, url):
    print(second)
    soup = BeautifulSoup(download_page(url), 'lxml')
    divs = soup.select('div#rg_s div.rg_meta.notranslate')
    if len(divs) == 0:
        return False

    for div in divs:
        img = json.loads(div.string.strip())
        data = {}
        data['first'] = first
        data['second'] = second
        data['img_url'] = img['ou']
        data['img_desc'] = img['pt']
        data['imgsrc_url'] = img['ru']
        # XHR
        data['relative_url'] = \
            "https://www.google.com/async/imgrc?" \
                f"imgurl={img['ou']}" \
                f"&imgrefurl={img['ru']}" \
                f"&tbnid={img['id']}" \
                f"&docid={img['rid']}" \
                f"&imgdii={img['id']}" \
            "&async=cidx:1,saved:0,iptc:1,iu:0,lp:1,_fmt:prog,_id:irc_imgrc1,_jsfs:Ffpdje"
        print(data, '\n')
        append_write(f'{src}/{first}/{second}.txt', json.dumps(data))
    return True


# 逐行从第一步中抽取，然后解析相关性
def parse_relative(src, dst):
    if not os.path.exists(dst):
        os.makedirs(dst)
    for file in os.listdir(src):
        with open(f'{src}/{file}') as f:
            for line in f:
                parse_relative_img(line, dst)
                # time.sleep(1)


def parse_relative_img(json_obj, dst):
    data = json.loads(json_obj)
    second = data['second']
    print(second)

    # 相关页
    html = download_page(data['relative_url'])
    relative_list = re.findall('<div.*?notranslate">(.*?)</div>', html)
    res = []
    for relative in relative_list:
        img = json.loads(relative)
        tmp = {
            'img_url': img['ou'],
            'img_desc': img['pt']
        }
        res.append(tmp)
    data['relative_img'] = res
    data.pop('relative_url', None)
    print(data, '\n')
    append_write(f"{dst}/{second}.txt", json.dumps(data))


# 解析图片描述
def parse_img_desc(url):
    soup = BeautifulSoup(download_page(url), 'lxml')
    try:
        h1s = soup.select('h1')
        if len(h1s) == 0: return ''
        res = list(h1s[0].strings)[0]
        return res.strip()
    except Exception as e:
        print(f'{url} h1 problem')
        print(e)
        return ''


def append_write(dst, txt):
    with open(dst, 'a') as f:
        f.write(txt)
        f.write('\n')


if __name__ == '__main__':
    field = 'navy'
    search_words = [
        'aircraft carrier',
        'military destroyer',
        'military corvette',
        'military battleship',
        'cruiser ship',
        'missile boat',
        'submarine'
    ]

    src = f'tmp/{field}'
    dst = f'output/{field}'
    pool = Pool(multiprocessing.cpu_count())
    for search_word in search_words:
        url = f'https://www.google.com/search?q={search_word}&source=lnms&tbm=isch'
        # 拆分任务
        # 第一步：解析页面和二级标签
        parse_page(search_word, url, src)

        # 第二步：解析相关性
        # parse_relative(f'{src}/{search_word}', f'{dst}/{search_word}')
        pool.apply_async(parse_relative, args=(f'{src}/{search_word}', f'{dst}/{search_word}'))
    pool.close()
    pool.join()
    print('All Finished!')
```

#### 反爬策略 vs 爬虫策略

反爬虫是使用技术手段 防止爬虫程序的方法

反爬：成功率越高成本越大，拦截率越高，误伤率越高

```
存在误伤，即 反爬技术 将普通用户识别为爬虫
如果误伤高 --- 效果再好也不能使用

	比如 限制 ip  === 用户的ip 一般都是 局域网内动态分配的，
    一个爬虫的ip 可能分配给 另一个 非爬虫的用户
    有效的方法： 可以在一段时间内 限制 ip，过一段时间 再把 ip释放
```

```
1.监控 某个时间 访问突然增加，ip相同，user-agent不是浏览器；限制 ip访问 (注意：不能封ip)
	（1）user-agent模拟，ip代理(ip代理池)

2 发现 ip 变化， 要求登录访问
	(2) 注册 账号, 每次请求 带 cookie

3  开发健全的账号体系，每个账号 权限不同
	(3) 多个账号 联合爬虫(维护账号池)

4 访问频繁 ,限制 ip频率
	(4) 模仿人的请求速度

5  弹出 验证码
	(5) 识别验证码

6  增加网页 内容的动态填充 ajax向后台请求
	(6) selenium + phantomJs 完全模拟浏览器
```

