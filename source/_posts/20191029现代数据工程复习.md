---
layout: post
title: "现代数据工程复习"
date: 2019-10-29 10:36
toc: true
comments: true
categories: 技术学习
tags: 
	- 复习
	- 分布式
---

#### 基本概念

##### 现代数据管理的需求

```
High performance高性能——对数据库高并发读写的需求
Huge Storage海量存储——对海量数据的高效率存储和访问的需求
High Scalability高可扩展性&& High Availability高可用性的需求

```

<!--more-->

##### 现代数据管理特点

```
1）数据的形式多样
    ——结构化、半结构化、非结构化
    ——文本、图像、视频、音频
    ——单模态、多模态
2）查询的需求
    ——文字匹配
    ——基于语义
    ——相似性匹配、排序
    ——聚类、分类、去冗余
3）应用和运行环境
    和网络（尤其是语义网）结合更加密切、更加直接:
    ——大量的数据直接来自网络，包括用于机器学习的训练数据和用于检索的数据。
    ——可能需要从网络中获取知识。
    ——与web应用集成（电子商务、搜索引擎、内容检索、情报分析）
    ——网络后台数据 、爬虫
4）处理的业务需求
    ——面向海量数据，TB、PB级别
    ——查询为主、更新为辅（日志更新例外）
    ——数据一致性可弱化
5）关键技术
    ——海量数据的分布存储、分布式并行处理
    ——相似性度量
    ——高维数据处理
    ——语义特征的获取
    ——语义知识的组织与映射
    ——模型的提出与训练
    ——查询扩展与反馈
    ——可视化
6）系统开放性
    ——分布式、易于扩充、低成本
    ——新的编程模型
```

##### 大数据特点（4V）

```
Volume：数据体量巨大（从TB级别，跃升到PB级别）；
Variety：数据类型繁多（网络日志、视频、图片、地理位置信息等等）；
Velocity：处理速度快（1秒定律，有别于传统的数据挖掘技术）；
Value：价值密度低。
Veracity：真实性——IBM。
Variability：易变性
```

##### 12306关键技术

```
（1）“双活数据中心”，由中国铁路总公司和中国铁道科学研究院两个双活数据中心同时工作和承担网站任务。
（2）“异步交易排队”，大家同时买票不可避免，‘异步交易排队’会建立一个有序排队形式，使网站承载压力可控。
（3）“分布式内存计算”，提高车票查询速度，该技术使得“不管是北京到太原30多趟车，还是北京到上海70多趟车，基本一两秒都可以出来”。
（4）“混合云计算”。除了12306自己建立的云计算平台，还会在业务高峰期，将查询业务推到云端
```

#### 多结构化数据存储与组织

```
Memcached(缓存系统、chunk块/slab块集、client实现分布式、一致性hash+虚拟结点、无冗余、处理“键值”集合)

DynamoDB(一致性hash+虚拟结点+固定、NWR读写协议、Vector Clock、Merkel Tree、Gossip协议)

Redis（缓存系统、持久化、node/client、slot槽、client缓存、多副本、P2P去中心化、无proxy、Gossip协议MEET/PING/PONG消息、ASKED/MOVED异常处理，处理五类Redisobject的键值集合）

Trinity（slave/proxy/client、slot槽、槽key向量、全局寻址表（slot数组）、slot内部hash表、2/8原理，图分割，restrictive模型，异步消息，TFS分布式文件系统，leader节点一致性广播，处理图数据）

Kad（ P2P覆盖网络、逻辑距离、近邻列表、查询即迭代和传播，处理文件）
```

#### Memcached

```
一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。更好的分配资源，更快速访问。
在内存中缓存数据和对象，为动态、数据库驱动网站提供更快的运行速度。LRU替换策略
减少读取数据库的次数，避免使用数据库应对高并发访问时磁盘开销和阻塞的发生。
分布式缓存，不同主机上的多个用户可同时访问， 解决了单机应用的局限。
```

##### 基本特征

```
使用自己的页块分配器
使用基于存储“键-值”对的hashmap哈希表。通过在内存里维护一个统一的巨大的hash表，Memcached能够用来存储各种格式（图像、视频、文件）
虚拟内存不会产生碎片，虚拟内存分配的时间复杂度可以保证为O(1)。
使用非阻塞的网络I/O，对内部对象实现引用计数。
不提供冗余（如复制hashmap条目），当某个服务器S停止运行或崩溃了，所有存放在S上的键-值对都将丢失。
```

![image-20200319111903166](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319111903166.png)

##### 运行原理

![image-20200319112319008](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319112319008.png)

##### 内存分配

```
早期的Memcached内存分配通过对所有记录进行malloc和free来进行。
    （1）容易产生内存碎片；
    （2）加重操作系统内存管理器的负担。 
改进措施：默认采用Slab Allocator机制分配、管理内存。

Slab Allocator基本原理：
    Chunk——按照预先规定的大小，将分配的内存分割成各种特定长度的块。
    slab class——尺寸相同的块分成组（chunk的集合）。
    分配到的内存不会释放，重复使用已分配的内存
    
空闲chunk的列表：memcached根据该列表选择chunk，选择最适合数据大小的slab。
	比如将100字节数据缓存到128字节chunk中，但Slab Allocator机制问题，浪费剩余28字节。
	
Lazy（惰性） Expiration
      记录超时不会释放已分配的内存，只是客户端无法再看见该记录， 其存储空间此时可重复使用。
      内部不实时监视记录是否过期，而是在get时查看记录的时间戳是否过期。
      不在过期监视上耗费CPU时间。
替换策略
    1）优先使用已超时的记录的空间。
    2）如果还存在追加新记录时空间不足的情况， 使用最近最少使用（LRU）机制替换已有缓存内容 （引用计数非零则不替换）。
```

![image-20200319112901409](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319112901409.png)

##### 分布式策略

Memcached通过将不同的键保存到不同的服务器上实现了分布式。服务器增多后键会分散，即使一台memcached服务器发生故障，也不影响其他缓存节点，系统依然能继续运行。

主要采用一致性hash+冗余节点+虚拟节点。

![image-20200319112157743](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319112157743.png)

```
Memcached的标准的分布式方法（对键的存储根据服务器台数的余数进行分散）：
1）求得键的整数哈希值；
2）除以服务器台数，根据其余数来选择服务器。
3）当选择的服务器无法连接时，rehash——将连接次数添加到键之后再次计算哈希值并尝试连接。

优点：方法简单，数据的分散性一般较好。
缺点：当添加或移除服务器时，缓存重组的代价大。
```

##### 一致性hash

```
一致性hash——改进的分布式方法
1）求出服务器节点的哈希值， 将其配置到0～2^32的圆上；
2）用同样的方法求出存储数据的键的哈希值并映射到圆上；
3）从数据映射到的位置开始顺时针查找，将数据保存到找到的第一台服务器上；
4）如果超过2^32仍然找不到服务器，就保存到第一台服务器上。
```

![image-20200319113623625](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319113623625.png)

```
添加一台服务器？
只有在环上增加服务器位置的逆时针方向第一台服务器之间的键会受到影响。有效地抑制了键的重新分布。
```

![image-20200319114123274](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319114123274.png)

##### 虚拟节点

```
使用一般的hash函数，服务器的映射地点的分布可能出现不均匀的情况。
为每个物理节点（服务器）在圆环上分配100～200个点，如节点A有A1和A2两个虚拟节点
抑制分布不均匀，最大限度地减小服务器增减时的缓存重新分布。
```

![image-20200319113809765](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319113809765.png)

##### Memcached架构

![image-20200319120212639](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319120212639.png)

#### DynamoDB

```
DynamoDB是Amazon（亚马逊）的key-value模式的存储平台，基本设计思想：
    为了达到高可用，牺牲一致性；
    在读数据的时候处理数据不一致的冲突；
    根据应用层的不同需求，指定不同的NRW值，协调可用性和一致性；
    去中心化的维护整个集群的成员及故障信息，采用Gossip同步。
    
DynamoDB技术特征：
	（1）数据分布
    	一致性hash+虚拟节点
    （2）支持数据的多副本写操作
    	节点临时性失效处理技术（sloppy Quorum & hinted handoff，一些副本不可用时，提供高可用和持久性的保证）；
    	节点永久性失效恢复技术（反熵 & Merkle trees，实现后台副本恢复）。
    （3）节点成员关系和失效检测
    	基于Gossip的成员协议和失效检测，避免用中心节点管理节点成员关系。
```

##### 改进一致性hash

![image-20200319114842551](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319114842551.png)

##### 时钟向量

![image-20200319120404641](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319120404641.png)

```
通过比较这些向量的大小，来确定事件发生的顺序：假如一个向量的所有分享量的count值都小于或等于另一个向量，可以认为后者并前者更“新”；否则，存在冲突。

Vector Clock实例：
设有3个节点，NRW协议中W=1,则R=N=3。
    假设一个写请求，第一次被节点A处理了。节点A会增加一个版本信息(A，1)，把这个时候的数据记做D1(A，1)。然后另外一个对同样key的请求又被A处理了，于是有D2(A，2)。
    此时D2是可以覆盖 D1的，没有冲突产生。现在假设D2传播到了所有节点(B和C)，所以B和C都持有数据D2(A，2)。
    接下来又一个请求被B处理了，生成数据D3(A，2;B，1) 。（因为这是一个新版本的数据，被B 处理，所以要增加B的版本信息）
    假设D3还未传播到C时，又一个请求被C处理记做，产生D4(A，2;C，1)。
    如果在这些版本没有传播开以前，发生读取操作，因为R=3，所以R会从所有三个节点上读，读到三个版本。A上的D2(A，2)、B上的D3(A，2;B，1)、C上的D4(A，2; C，1)。依据Vector Clock算法，D2已经是旧版本（其B、C分量均为0），可以舍弃，但是D3和D4都是新版本，需要应用自己去合并。
```

**Vector Clock的无限增长问题：**

现实生活中，如果有很多的决策者，相当于有很多的客户端，整个向量时钟的长度就无限制增长了，这对于存储系统来说，需要想办法解决。主要方法有服务器向量和向量时钟剪枝。

**（1）服务器向量**

不要用client来标识向量空间，用server来标识向量空间。因为server的数量是可控的，向量标签不再是客户端，而是用server标识。

问题：会因为网络传输延时而丢失数据。如下图：

![image-20200319121017894](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319121017894.png)

**（2）时钟向量剪枝**

```
Riak系统用四个参数来避免向量时钟空间的无限增长：
small_vclock和big_vclock参数标识向量时钟的长度，如果长度小于small_vclock就不会被剪枝掉，如果长度大于big_vclock就会被剪枝掉。
young_vclock和old_vclock参数标识存储这个向量时钟时的时间戳，剪枝策略同理，大于old_vclock的才会被剪枝掉。

向量剪枝尽量只丢掉一些向量时钟的信息，而不是丢掉实实在在的数据。
    但是有一种情况会有问题：一个客户端保持了一个很久之前的向量时钟，然后继承这个向量时钟并提交了一个数据，此时会产生冲突（因为服务器已经没有这个很久之前的向量时钟信息了，可能已经被剪枝掉了），所以客户端提交的此次数据，在服务端无法找到一个祖先。此时Riak会创建一个sibling。
    所以，剪枝策略是一个tradeoff权衡，一方面是无限增长的向量时钟的空间，另一方面是偶尔的会有“false merge”，产生兄弟数据，但不会丢失数据。
    从这个意义上看，防止向量时钟空间的无限增长，剪枝策略优于用server标识向量时钟的策略。
```

![image-20200319121054927](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319121054927.png)

##### 节点临时性失效——Sloppy quarum

Sloppy quarum（草率仲裁，草率的法定人数）：

![image-20200319115015235](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319115015235.png)

```
Hinted Handoff技术：为了保证每次都能写到W个副本，读到R个副本，我们每次读和写都是发送给N个节点。如果这N个节点有节点失效，那么往后继续找一个不同的节点，暂时的代替失效的节点，当该后续节点定期监测到故障节点恢复，则将暂时代为保管的数据写回复活节点。
例：N=3，某数据的preference list是节点A、B、C。若A节点失效，则对该数据的写请求将发送到节点B、C、D上。D暂时取代A的角色，那些原本应该写到A上的数据存放在D中的一个特定的文件夹中（意味着这些数据不是D本该拥有的，而是别的节点的）。D上会启动一个线程定期检查A的状态，当发现A恢复后，就将D上存放的这些A的数据写回到A。
该策略保证了节点失效时系统的高可用和数据持久性。
```

##### 节点永久性失效——Merkle tree

```
Dynamo实现了反熵协议(anti-entropy，基于Gossip的一致同步) 来保持副本同步。
    当故障发生或者有节点加入、离开集群时，都涉及分片的拷贝和传输，因此希望能够快速检查分片中内容是否相同，并通过仅发送不同的部分来减少数据传输量。
    为了更快地检测副本之间的不一致性，并且减少传输的数据量和反熵过程中磁盘读取的次数，Dynamo采用 Merkle Tree技术。
    
Merkle tree： 
    每个叶子节点对应一个数据项，并记录其hash值 ；
    每个非叶子节点记录其所有子节点的hash值。 
	Dynamo为每一个分片维护一个Merkle Tree，需要比较分片是否相同时，自根向下的比较两个Merkle Tree的对应节点，可以快速发现并定位差异所在。
	主要用于：文件校验（BitCommit，BitTorrent种子）；副本同步（DynamoDB）；可信计算；区块链

Merkle Tree的主要优点是树的每个分支可以独立地检查，而不需要下载整个树或整个数据集。此外，MerkleTree有助于减少为检查副本间不一致而传输的数据的大小。
例：如果两树的根哈希值相等，且树的叶节点值也相等，那么节点不需要同步。如果不相等，则意味着一些副本的值不同。在这种情况下，节点可以交换子节点的哈希值，该处理一直进行到树的叶子，此时主机可以识别出“不同步”的 key。
```

##### 成员信息及故障检测——Gossip

```
Gossip协议传播成员变动
    Dynamo集群中的每个节点都会维护当前集群的成员及节点不可达等信息，这些信息通过Gossip协议传播到整个集群；客户端可以通过任意一个节点获得并维护这些成员信息，从而找到自己要访问的数据。
	Dynamo使用一个基于Gossip的协议传播成员变动，并维持成员的最终一致性：每个节点每隔一秒随机选择另一个节点，两个节点协调他们保存的成员变动历史。
    新节点加入时选择自己负责的虚拟节点，并将其虚拟节点表保存到磁盘，之后与其他的节点通过Gossip协议交换协调他们的虚拟节点表。这样，每个节点都知道全局的虚拟节点表。
```

##### 读写流程

```
客户端请求最终交给preference list中的一个节点处理，该节点称为coodinator 。Dynamo采用类似Quarum的方式保证数据正确，即W+R>N。 
Put流程： 
（1）coodinator生成新的数据版本，及vector clock分量 ；
（2）本地保存新数据 ；
（3）向preference list中的所有节点发送写入请求 ；
（4）收到W-1个确认后向用户返回成功 。
Get流程 ：
（1）coodinator向preference list中所有节点请求数据版本 ；
（2）等到R-1个答复 
（3）coodinator通过vector clock处理有因果关系的数据版本 ；
（4）将不相容的所有数据版本返回用户。
```

##### NWR理论

```
NWR理论（WernerVogels在讲“EventuallyConsistent”时提出）。设一个存储系统有如下属性：
N=每个数据的副本数
W=每次写操作时，必须同步确认写成功的副本数
R=每次读操作时，需要读取的副本数
则当W+R>N时，该存储系统可以提供强一致性。
强一致性等价于R中至少包含一个最新的副本，即(R-(N-W))>0，即W+R>N。
```

#### Redis

Redis是一个开源的key-value存储系统，将大部分数据存储在内存中

![image-20200319132526526](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319132526526.png)

##### Redis内存管理

```
Redis为了方便内存的管理，在分配一块内存之后，会将这块内存的大小存入内存块的头部。
real_ptr是调用malloc后返回的指针，内存块的大小size存入头部（占据为 size_t类型的内存），然后返回ret_ptr。
当需要释放内存时，ret_ptr被传给内存管理程序，算出 real_ptr的值，然后将real_ptr传给free释放内存。

Redis通过定义一个数组zmalloc_allocations[]来记录所有的内存分配情况。
这个数组的长度为ZMALLOC_MAX_ALLOC_STAT。数组的每一个元素代表相应大小内存块被分配的个数（内存块的大小为该元素的下标）
	例如：zmalloc_allocations[16]代表已经分配的长度为16bytes的内存块的个数。

有一个静态变量 used_memory记录当前分配内存的总和。
总体上看，Redis采用的是包装的malloc/free。
调用malloc时，malloc并不是严格按照参数的值来分配内存。
例如基于内存对齐等方面的考虑，程序只请求一个byte时malloc可能会分配4个byte。
```

![image-20200319133105440](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319133105440.png)

##### Redis的持久化机制

```
Redis虽然是基于内存的存储系统，同时也提供内存数据持久化的机制，有两种持久化策略：
      RDB快照、
      AOF日志。
1）RDB快照：将当前数据的快照存成一个数据文件，从而持久化。
      RDB的实现借助了fork命令的copy on write机制。在生成快照时，将当前进程fork出一个子进程，然后在子进程中循环所有的数据，将数据写成为RDB文件。
      可靠性：当生成一个新的RDB文件时，Redis生成的子进程先将数据写到一个临时文件中，然后通过原子性系统调用rename将临时文件重命名为RDB文件，这样在任何时候出现故障，RDB快照文件总是可用的。
	  生成时机：可以通过save指令配置RDB快照生成的时机（例如当10分钟以内有100次写入就生成快照，或者1小时内有1000次写入就生成快照，或者多个规则一起实施）。
      可用性：开启RDB的代价不高，但是RDB文件中的数据并不是全新的，从上次RDB文件生成到Redis停机这段时间的数据将丢失。
      
2）AOF日志（Append Only File）：一个追加写入的日志文件，与一般数据库的bin log不同，AOF文件是可识别的纯文本，内容是一个个导致数据变化的Redis标准命令。

优化策略：AOF文件会越来越大，所以Redis提供了AOF rewrite功能，就是重新生成一份AOF文件，文件中一条记录的操作只会有一次，去掉之前叠加的操作。

Rewrite的AOF文件的生成过程类似于RDB：
    ①fork一个进程，遍历数据，写入新的AOF临时文件。在写入新文件的过程中，所有的写操作日志还是会写到原来老的AOF文件中，同时还会记录在内存缓冲区中。
    ②当重完操作完成后，还要将所有缓冲区中的日志一次性写入临时文件。
    ③调用原子性的rename命令用新的AOF文件取代老的AOF文件。
```

##### Redis Cluster分布式存储

```
Redis Cluster采用了P2P机制，没有Proxy层，客户端将key的请求转发给合适的nodes。
Client保存集群中nodes与keys的映射关系（slots），并保持此数据的更新，所以通常Client能将请求直接发送给正确的nodes。
Clients与每个nodes保持链接，所以请求延迟等同于单个节点，不会因为Cluster的规模增大而受到影响。
由于没有Proxy层，Client请求的数据无法在nodes间merge。
Redis核心面向K-V数据存储，没有scan类型（sort，limit，group by）的操作。

Redis槽（slot）：集群将key分成16384个slots（hash 槽），slot作为数据映射的单位。
Keys到slot的映射：
    HASH_SLOT = CRC16(key) mod 16384。其中CRC16是一种冗余码校验和，将字符串转换成16位的数字。

每个节点持有16384个slots中的一部分。
      ↓
Redis Cluster最多支持16384个nodes（每个nodes持有一个slot）。

Redis集群中的各个节点通过Gossip协议来交换各自关于不同节点的状态信息，协议由三种消息实现：
每次发送MEET、PING、PONG消息时，发送者都从自己的已知节点列表中随机选出两个节点的信息(可以是主节点或者从节点) 保存到两个clusterMsgDataGossip结构中
```

![image-20200319133411353](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319133411353.png)

![image-20200319154231111](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319154231111.png)

##### Redis Cluster的复制机制

```
为了保证单点故障下的数据可用性，Redis Cluster引入了Master节点和Slave节点
每个Master节点有两个用于冗余的Slave节点。 
集群中任意两个节点宕机都不会导致数据不可用。若Master节点退出，集群会自动选择一个Slave节点成为新的Master节点。
```

![image-20200319154101670](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319154101670.png)

##### Redis Cluster的核心数据结构

```
clusterState：集群状态，每个节点都保存着这样一个状态，记录它们眼中集群的状态，里面含有一个slot数组，其中myself指针变量指向本节点的clusterNode。

clusterNode：记录节点的角色（主节点或从节点）、状态（是否在线）、slot属性（二进制位数组）。
    slot属性是数组长度为16384/8=2048个字节，包含16384个二进制位。
    Master节点用bit来标识自己是否拥有某个槽。
    所有槽的指派信息保存在clusterState.slots数组内。
    程序要检查槽i是否已经被指派，或者要取得负责处理槽i的节点，只需要访问clusterState.slots[i]的值，复杂度仅为O（1）。

clusterLink：包含与其他节点通讯所需的全部信息。
```

![image-20200319154634313](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319154634313.png)

##### Redis Cluster数据请求流程

![image-20200319154947549](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319154947549.png)

![image-20200319155008420](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319155008420.png)

![image-20200319155028477](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319155028477.png)

##### Multi-key和hash tags

```
Multi-key操作（一次RPC调用中需要进行多个key的操作，如Set类型的交集、并集等）
    ↓
这些key必须属于同一个node。

“hash tags”机制：每个key都可以包含一个自定义的“tag”，存储时根据tag计算该key应分布在哪个node上（不是使用key计算，但是存储层面仍然是key）。
该机制可强制某些keys被保存在同一个节点上，以便于进行“multi-key”操作，
```

#### Trinity

```
微软的图处理引擎，基于分布式内存的云系统，能够有效支持针对web规模图数据的在线和离线处理任务。
在分布式缓存的基础上实现了对图数据的全局寻址，可有效支持随机存取。
默认前提条件：内存成本足够低、网络速度足够高。

简单任务：pageRank、shortest path发现（基于LandMark节点）…
复杂任务：多层次图分割…

Trinity本身不带有复杂的图计算模型，只是一个分布式的key-value系统，但是提供灵活的数据定义和处理建模能力
     ↓
有利于和图计算应用系统相集成。

Trinity系统由三类节点构成：slave、proxy和client。
    Slave节点：存储一部分图数据，执行图计算任务。图计算任务包括向其他各类节点收发消息。
    Proxy节点：系统中的可选节点，不包含数据，只处理消息。作为client和slave节点的中间层，也用作消息聚集节点，可汇总来自多个slave的消息。
    Client节点：用户接口层，通过API和slave以及proxy节点通讯。
```

![image-20200319155658445](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319155658445.png)

#### Kad

```
Kademlia(简称Kad)，一种典型的结构化P2P覆盖网络(应用层网络）。
信息的存储：以哈希表条目形式分散存储在各节点上。
    ↓
全网构成一张巨大的分布式哈希表

检索：通过Kademlia协议查询key值对应的value（不必关心value所在节点位置）。
应用：eMule、BitTorrent等P2P文件交换系统的检索协议。

网络集群存储、维护两张分布式哈希表：关键词字典、文件索引字典。
关键词字典：关键词→其所对应的文件名称及相关信息，key=关键词字符串的160比特SHA1散列，value为一个三元组列表 (文件名，文件长度，文件的SHA1校验值) 。
文件索引字典：文件信息→文件的拥有者(下载服务提供者)，key=文件的SHA1校验值，value也是一个三元组列表 (拥有者IP，下载侦听端口，拥有者节点ID)。

字典条目均分布式存储于参与Kad网络的各节点中，其存储和交换无需集中索引服务器的参与。
     ↓
    1）提高了查询效率
    2）提高了文件交换系统的可靠性。

每一个节点有一个专属ID（一个160bit的整数），由节点自己随机生成（可以认为ID具有唯一性）。
距离为两个ID的二进制异或值。
    两个节点的ID分别为a与b，则有：
    distance=a XOR b。

Kad网络规定：条目依据其key值被复制到目标节点ID距离最近的k个节点中。
k取值准则——任意选择至少k个节点， k的典型取值为20，它们在任意时刻同时不在线的几率几乎为0。

为了实现较短的查询响应延迟，在条目查询的过程中，任一条目可被cache到任意节点之上。
时效性：考虑条目在节点上存储的时效性，越接近目标结点保存的时间将越长。

节点之间的距离取异或值
     ↓
对于同一个key值的所有查询都会逐步收敛到同一个路径上，而不管查询的起始节点位置如何。
     ↓
沿着查询路径上的节点都缓存相应的<key，value>对，可以有效减轻存放热门key值节点的压力，加快查询相应速度。

kad节点维护
每一个节点均维护160个list，每个list称为一个k-桶(k-bucket) 。
    第i个k-桶的内容：记录当前节点已知的与自身距离为2^i~2^(i+1)的其他节点的网络信息(NodeID，IP地址，UDP端口)。
    一个k-桶最多存放k个对端节点信息，桶中节点信息按访问时间排序（最早访问的在头部）。

List（k-桶）的更新原则：
    1）目标节点信息已经在list中，将其移至队尾；
    2）list未满，且目标节点不在其中，其信息将直接添入list队尾；
    3）list已满，先检查队首节点是否仍有响应，如果有，则队首节点被移至队尾，目标节点被抛弃；如果没有，则抛弃队首节点，将最新访问的节点信息插入队尾。

每个k桶覆盖距离的范围成指数关系增长
     ↓
离自己近的节点的信息多，离自己远的节点的信息少
     ↓
路由查询过程是收敛的。
    由于采用指数方式划分区间，有关研究证明：对于一个有N个节点的Kad网络，最多只需要经过logN步查询就可以准确定位到目标节点。
K桶的设计初衷：维护最近最新见到的节点信息更新，对于某个需要查找的特定ID节点N，可以从当前节点的k桶中迅速的查出距离N最近的若干已知节点。

Kad寻找节点
查找与目标节点网络距离最近的k个节点所对应的网络信息(NodeID，IP地址，UDP端口)。
1）发起者从自己的k-桶中选出若干距离目标ID最近的节点，并向它们同时发送异步查询请求；
2）被查询节点收到请求后，从自己的k-桶中找出自己所知的目标ID的若干近邻返回给发起者；
3）发起者收到返回信息后，再次从当前已知的近邻节点中选出若干未被请求的，并重复步骤1。
重复上述过程2）~3）直至无法获得k近邻的更新时停止。
在查询过程中没有及时响应的节点将立即被排除。

Kad条目搜索
搜索发起方以迭代方式不断查询距离key较近的节点
↓ 
直至查询路径中的任一节点返回所需查找的value。
系统优化：
搜索成功后发起方可选择将条目作为cache存储到查询路径的多个节点中，条目cache的超时时间与节点的距离呈指数反比关系。

Kad新节点加入
1）获知一个已经加入Kad网络的节点信息(记为节点I)，并将其加入自己的k-buckets；
2）向I节点发起一次针对自己ID的节点查询请求，从而通过节点I获取一系列与自己邻近的其他节点信息；
3）刷新所有的k-bucket，保证自己获得最新的节点信息。（自身信息通过查询被传播出去）
```

#### 实际案例解读（淘宝）

##### 个人网站LAMP

```
LAMP(Linux+Apache+MySQL+PHP)
    一个数据库进行所有的读写操作
         ↓
    拆分成一个主库、两个从库，读写分离。

    好处：1）存储容量增加；
             2）有了备份；
             3）安全性增强；
             4）读写分离使读写效率提升。
```

![image-20200319134140600](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319134140600.png)

```
    上述方案随着访问量和数据量的飞速上涨，问题很快出现。
1）数据库锁表，MySQL当时是第4版的，使用默认的存储引擎MyISAM   →  读数据时会锁表。
    主库往从库写数据时会对主库产生大量的读操作，使主库性能急剧下降。
    高访问量时，数据库支撑不住。 (Oracle在写数据时会有行锁，读数据时没有)
2）数据容量和安全性问题，MySQL当时不稳定。
```

##### ORACLE本地后台DB+连接池

```
更换后台数据库
          ↓
    Oracle 的连接池可有效增强数据库的并发访问能力，但 PHP pearDB是放在Apache上的，没有连接池功能，因此每一个请求都会对数据库产生一个连接。 (Java 语言有 Servlet 容器，可以存放连接池)
          ↓
    eBay 在 PHP 下面采用了一个从BEA购买的连接池的工具，较贵。采用替代方案，一个开源的连接池代理服务 SQLRelay(http://sourceforge.jp/projects/freshmeat_sqlrelay ) 提供连接池功能。

```

![image-20200319161038183](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319161038183.png)

##### ORACLE+NAS+RAC+连接池

```
上述方案中数据是本地的，优化工作包括DBA对 Oracle进行性能调优和SQL语句优化。
	↓
数据量继续增大，本地存储难以支撑。
	↓
采用NetApp 的 NAS(NetworkAttached Storage：网络附属存储) 存储作为数据库的存储设备，加上 Oracle实时应用集群 (RAC，Real Application Clusters，)来实现负载均衡。
	↓
隐患：1）NAS的NFS(Network File System)协议传输的延迟很严重；
2）RAC的稳定性；
3）SQLRelay容易死锁。


NAS(网络附属存储)
NAS的典型组成是使用TCP/IP协议的以太网文件服务器，数据处理是“文件级”（file level），可以把NAS存储设备附加在已经存在的太网上。
NAS主要应用于文件共享任务。
在数据库应用中应谨慎使用 NAS 解决方案，通常限于以下条件：
    大多数数据存取为只读方式、
    数据库小、
    存取量低、
    不指定预期性能
在这种情况下，NAS解决方案减少用户整体存储成本较有效。

SAN（Storage Area Network，存储区域网）
SAN是一个高速的子网，子网中的设备可以从主网卸载流量。通常SAN由RAID阵列连接光纤通道组成，SAN和服务器和客户机的数据通信通过SCSI命令而非TCP/IP，数据处理是“块级”（block level）。
SAN利用光纤通道协议（FCP，Fibre Channel Protocol ）上加载SCSI协议来达到可靠的块级数据传输。
    ↓
高性能的光纤通道交换机和光纤通道网络协议是SAN的关键。
在一些关键应用中（尤其是多个服务器共同读取大型存储设备），传输块级数据要求必须使用SAN。

NAS vs. SAN
1）NAS是一台特殊的含有大硬盘空间的计算机，连接在以太网上，其它计算机通过网络映射硬盘使用该空间。
    SAN是一种容易扩容的光纤通讯的磁盘阵列机，是多台服务器共享使用多台阵列机，可以安装各种软件，可跨平台。
2）SAN是光纤协议，NAS是TCP/IP协议。NAS是利用现有网络，SAN是在sever端再架设一个网络。
3）NAS以文件方式访问数据，而SAN以sectors方式访问数据。
    SAN对于高容量块级数据传输有明显的优势，易扩展且管理高效，可运行关键应用（如数据库、备份等）。
    NAS更加适合文件级别的数据处理。可作为日常办公中需要经常交换小文件的存储配置（如存储网页）。
4）SAN更多的是强调：范围+高效。
	NAS主要强调：共享。
```

##### ORACLE+SAN+RAC+连接池

```
ORACLE+NAS+RAC+连接池：NAS的文件级I/O协议及传输的延时，影响甚至限制了系统的读写性能优化。
    数据容量的持续增加。
          ↓
ORACLE+SAN+RAC+连接池
```

##### ORACLE+SAN+小型机+连接池+人工维护

```
RAC 出问题
	↓
更换为小型机。

SQLRelay版本内部处理逻辑不合适，数据库连接的代理服务经常死锁。
	↓
人工kill进程再重启服务（“后来干脆每天睡觉之前先重启一下”）。
```

##### SUN JAVA(Weblogic+EJB)

```
为了缓解数据库的压力，商品查询和店铺查询放在搜索引擎上。
DBMS稳定——ORACLE，小型机
存储扩容——SAN
程序框架优化——MVC
连接池稳定——Weblogic
数据访问负载拆分——查询引擎
系统负载分割——控制层EJB、持久层iBATIS的OR-Mapping
```

![image-20200319162319255](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319162319255.png)

##### (Weblogic+EJB)+DBRoute

```
一台Oracle处理能力有限：连接池有数量限制，查询速度跟容量成反比。数据量上亿、查询量上亿时，到达极限。
          ↓
    多用几个Oracle 数据库，即”分库分表“。
          ↓
    用户信息按ID分到两个数据库中 (DB1/DB2)，商品信息和卖家也分到两个对应的数据库中，商品类目等通用信息放在第三个库中 (DBcommon)。
          
    买家的操作（关键字查询、分页、按时间排序）？
              ↓
   数据库路由程序框架DBRoute（自行开发）。
```

![image-20200319162628467](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319162628467.png)

##### Jboss+Spring+cache+分库分表

```
随着数据量的继续增长，到了2005年，商品数有 1663 万，PV(page view)有8931万，注册会员有1390万，庞大的访问量和数据量继续给数据和存储带来压力。
      ↓
通过缓存（一个基于 Berkeley DB 的开源的缓存系统，用于缓存变化较少的只读信息）和 CDN(内容分发网络)提升性能。
      ↓
到2007年，应用服务器达到几百台（运行Java的 WebLogic），而WebLogic成本较高，甚至比应用服务器还贵。
      ↓
替换掉WebLogic，采用开源的JBoss，降低成本。
```

![image-20200319162944577](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319162944577.png)

##### TFS+OceanBase

```
考虑两点原因：查询的负载；存储有限。不能让所有的商品一直存放在主库中。
   ↓
商品由卖家选择7天或14天有效期，到期之后就下架，须重新发布上架，之后变成新的商品(ID改变)。
   ↓ 
商品之前的好评没有和新商品ID关联。

拆分商品和交易：一个商家的一种商品有唯一的ID（不论上下架与否）。
	↓
卖家改价格、库存，已成交的信息怎么处理?
	↓
快照文件：买家每交易一次都记录下商品的快照信息。
	↓
内容增多，存储成本增加。
后期开发了海量文件系统TFS和海量数据库OceanBase。

淘宝网要存储很多文件：一个商品几张图片，每一张图片要生成几张规格不同的缩略图，商品有描述信息，还有交易快照。
    2010年，淘宝网有286亿个图片文件，图片的访问流量占整体流量的90%以上。图片平均大小为17.45KB，小于8K的图片占图片总数61%，占系统容量的11%。
	↓
    对于大量高并发访问的系统，困难在于大规模的小文件存储与读取（磁头要频繁的寻道和换道，容易导致较长的读取延时）。

2007年之前淘宝的文件存储一直采用商用系统（NetApp公司的文件存储系统）。
图片文件数量以每年3倍的速度增长
      ↓ 
后端商用存储系统从低端到高端不断迁移
      ↓ 
2006年，NetApp公司最高端的产品也不能满足需求。
      ↓
自行开发一套针对海量小文件存储的文件系统TFS。

2007年google公布了GFS( google file system )的设计论文
    ↓ 
淘宝开发了自己的图片存储系统TFS( taobao file system )。
2007年6月上线运营，集群规模达到了200台PC Server，文件数量上亿。
系统部署存储容量140TB，实际使用50TB;单台支持随机IOPS(Input/Output Operations Per Second)200+，流量3MBps。

TFS
集群组成：
  两台Name Server，作文件系统管理结点，互为双机备份。
  多台Data Server。
数据组织：以block文件(一般64M一个block)的形式存放数据文件，block有多个副本。
元数据：文件名内置元数据信息（如图片大小、时间、访问频次、所在的逻辑块号等），元数据信息很少，仅仅需要一个fileID从而定位文件。
用户自己保存TFS文件名与实际文件的对照关系——减少元数据量。
1）一些文件信息都隐藏在文件名中；
2）目录树开销很大。
          ↓
放弃了传统的目录树结构
          ↓
放弃目录树结构使整个集群的可扩展性极大提高，和目前业界的“对象存储”类似。


OceanBase(海量数据库)
虽然数据库系统数据量十分庞大（可能几十亿、几百亿条甚至更多），但一段时间(例如一天)的修改量并不大（通常不超过几千万条到几亿条）
          ↓
增量数据(UpdateServer) + 基线数据(ChunkServer)
          ↓
增量数据： OceanBase使用单台服务器(UpdateServer)记录一段时间的修改增量，其存储以内存表(memtable)为主，SSD(固态盘)为辅。
基线数据：在增量数据时间段内保持不变的数据称为基线数据，存储于多台服务器(ChunkServer)，类似于分布式文件系统。
```

![image-20200319171720471](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319171720471.png)

#### GFS

![1571990774449](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571990774449.png)

```
设计思想与基本概念
    机群容错、chunk、namespace
体系架构
    chunkserver/master/client，控制流/数据流，心跳信号/轮询
    元数据操作日志/checkpoint，master复制/影子
数据访问
    读/写流程（租约），记录追加（原子操作），一致性模型
    记录标识符，版本（租约），版本及checksum校验，快照
空间管理
    Namespace锁，chunk创建/移动/回收
```

##### GFS设计思想与基本概念

```
组件失效被认为是常态事件，而不是意外事件。任何时间都可能有某些组件无法工作、无法从目前的失效状态中恢复
以通常的标准衡量，我们的文件非常巨大。
绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。
应用程序和文件系统 API 的协同设计提高了整个系统的灵活性.
放松了对一致性模型的要求，引入了原子性的记录追加操作，从而保证多个客户端能同时进行追加操作，不需要额外的同步操作来保证数据的一致性。
```

##### GFS架构

一个 GFS 集群包含一个单独的 Master 节点、多台 Chunk 服务器，并且同时被多个客户端访问.

![1571972900106](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571972900106.png)

```
GFS 存储的文件都被分割成固定大小的 Chunk。在 Chunk 创建的时候，Master 服务器会给每个 Chunk 分配一个不变的、全球唯一的 64 位的 Chunk 标识。

Namespace(名称空间)：GFS提供了一套类似传统文件系统API的接口函数（没有严格按照POSIX等标准API的形式实现），文件以分层目录的形式组织，用路径名来标识。 在逻辑上，GFS的名称空间就是一个全路径和元数据映射关系的查找表。
利用前缀压缩，namespace表高效的存储在内存中。
在存储名称空间的树型结构上，每个节点（绝对路径的文件名或绝对路径的目录名）都有一个关联的读写锁。

Chunkserver：Chunk服务器把Chunk以linux文件的形式保存在本地硬盘上，根据指定的Chunk标识和字节范围来读写块数据。
可靠性→ 每个块会复制到多个chunkserver上（缺省3个存储复制节点）。
复制级别：用户可以为不同的文件命名空间区域设定不同的复制级别。

Master节点：GFS分布式文件系统中的总控节点，管理所有的文件系统元数据
Master节点使用心跳信息周期地和每个Chunk服务器通讯：
	（1）发送指令到各个Chunk服务器；
    （2）接收Chunk服务器的状态信息。
执行所有的namespace操作。
管理整个系统里所有Chunk的副本
    创建新Chunk和它的副本
    决定Chunk的存储位置
    在所有的Chunk服务器之间进行负载均衡
    协调各种系统活动以保证Chunk被完全复制
    回收不再使用的存储空间
Master管理的内容
    所有的文件系统元数据：
        Namespace(文件和Chunk的命名空间)
        文件和Chunk的映射信息
        Chunk当前的位置信息
        访问控制信息
    系统范围内的活动：
        Chunk租约管理
        孤儿Chunk(orphaned chunks)的回收
        Chunk在Chunk服务器之间的迁移
Master不持久保存Chunk位置信息(因为在一个有数百台服务器的集群中，这类事件会频繁的发生：有Chunk服务器加入/离开集群、更名、失效、重启。只有Chunk服务器才能最终确定某个Chunk是否在它的硬盘上)
     ↓
在启动或者有新的Chunk服务器加入时，向各个Chunk服务器轮询它们所存储的Chunk信息，并通过周期性的心跳信息监控Chunk服务器的状态。

Client：GFS客户端实现了GFS文件系统的API接口函数、应用程序与 Master 节点和 Chunk 服务器通讯、以及对数据进行读写操作。代码以库的形式被链接到客户程序里。
Client应用程序同时与Master节点和Chunk服务器通讯，对数据进行读写操作。
         ↓
    客户端和Master节点的通信只获取元数据；
    所有的数据操作都是由客户端直接和Chunk服务器进行交互的。
```

##### 日志与灾难恢复

```
Master持久化的内容——操作日志
操作日志登录两种类型的元数据变化（命名空间、文件和Chunk的对应关系）：
（1）同时会以操作日志的方式记录在操作系统的系统日志文件中（存储在本地磁盘）；
（2）该日志同时被复制到其它的远程Master服务器上。
     ↓
    系统能简单可靠的更新Master服务器的状态，不用担心Master服务器崩溃导致数据不一致的风险。

操作日志记录元数据变更历史的作用：
    1）是元数据唯一的持久化存储记录；
    2）是判断同步操作顺序的逻辑时间基线
        （逻辑日志序号作为操作发生的逻辑时间，类似于事务系统中的LSN）。
        文件和Chunk（及其版本）都由创建它们的逻辑时间唯一的、永久的标识。

记录日志机制
1）确保日志文件是完整的且元数据的变化被持久化后，日志才对客户端是可见的。否则，即使Chunk本身没有出现问题，仍有可能丢失整个文件系统（或者客户端最近的操作）。
2）日志会复制到多台远程机器，只有相应的日志记录写入本地以及远程机器的硬盘后，才会响应客户端的操作请求。
3）Master服务器会收集多个日志记录后批量处理，以减少写入磁盘和复制对系统整体性能的影响。

灾难恢复时，Master通过重演操作日志把文件系统恢复到最近的状态。类似REDO

日志的问题：从日志头开始恢复（日志过长）?
	解决方法：checkpoint，减少重演操作的日志量。

日志增长到一定量   →   Master对系统状态进行一次Checkpoint，将所有的状态数据写入一个Checkpoint文件，并删除之前的日志文件。

系统恢复：Master从磁盘上读取最新的Checkpoint文件，重演Checkpoint之后的有限个日志文件。

Checkpoint文件存储：压缩B树形式。可直接映射到内存，在用于命名空间查询时无需额外的解析，从而大大提高恢复速度，增强可用性。

checkpoint过程不阻塞正在进行的修改操作：
    Master使用独立的线程切换到新的日志文件和创建新的Checkpoint文件。
    新的Checkpoint文件包括切换前所有修改（一个包含数百万个文件集群创建Checkpoint文件需1分钟左右）。
    创建完成后，Checkpoint文件被写入本地和远程硬盘。
    旧的Checkpoint文件和日志文件可以被删除，但是为了应对灾难性的故障，通常会多保存一些历史文件。

Master状态复制
	为保证Master服务器的可靠性，Master的状态（所有的操作日志和checkpoint文件）会被复制到多台机器上。
    对Master状态的修改操作能够提交成功的前提：操作日志写入到Master的备份节点和本机的磁盘。
    Master服务进程负责所有的系统状态修改操作（包括后台的服务，如垃圾回收等），当它失效时可以立刻切换并继续提供服务。

Master切换机制：
客户端使用类似DNS别名的规范名字访问Master（如gfs-test）节点。
如果Master进程所在的机器或者磁盘失效
       ↓
处于GFS系统外部的监控进程在其它的存有完整操作日志的机器上启动新的Master进程，通过更改别名的实际指向，客户端访问新的Master节点。

“影子”Master节点
    GFS中还有若干 “影子”Master节点，在“主”Master服务器宕机时提供文件系统的只读访问。
    影子的数据可能比“主”Master更新要慢（通常不到1秒）。
    影子Master上可能过期的是文件的元数据
    因为文件内容是从Chunk服务器上读取的，应用程序不会发现过期的文件内容。
“影子”Master为保持自身状态是最新的
    ↓
读取当前正在进行的操作的日志副本，并依照和主Master相同的顺序来更改内部的数据。

“影子”Master如何得到（位置、节点信息）
主Master因创建和删除副本导致副本位置信息更新时，“影子”Master和主Master通信来更新自身状态。
其余时间，和主Master一样，“影子”Master在启动时从Chunk服务器轮询数据（之后定期pull数据），包括Chunk副本的位置信息。
“影子”Master也会定期和Chunk服务器“握手”来确定它们的状态。
```

##### 快照机制

```
快照机制（用途）
应用请求：对一个文件或目录树做一个拷贝
    ↓
    快照操作
    ↓   快
    几乎瞬间完成，几乎不会对正在进行的其它操作造成任何干扰。

    用户可使用快照迅速创建一个巨大数据集的分支拷贝，或使用快照操作备份当前状态，之后可以轻松的提交或回滚到备份时的状态。

实现技术：copy-on-write。
1）Master节点收到快照请求，取消做快照的文件的所有Chunk的租约（后续对这些Chunk的写操作都必须与Master交互以找到租约持有者，从而给Master一个率先创建Chunk的新拷贝的机会）。
2）租约取消或过期之后，Master把操作以日志的方式记录到硬盘上，然后通过复制源文件或者目录的元数据的方式，把这条日志记录的变化反映到内存的状态中。新创建的快照文件和源文件指向完全相同的Chunk地址，Chunk引用计数变为1。

快照操作后，当客户机第一次想写入数据到相关Chunk C时：
1）它会先发送请求到Master查询当前的租约持有者。
2）Master此时发现Chunk C的引用计数大于1，会选择一个新的Chunk句柄C’。
3）Master要求每个拥有Chunk C当前副本的Chunk服务器在本地创建一个叫做C’的新Chunk。
    本地复制使得写请求的处理方式和任何其它Chunk几乎没有区别。
    Master节点确保新Chunk C’的一个副本拥有租约并回复客户机，客户机得到回复后就可以正常的写该Chunk C’。
```

##### GFS缓存

```
Master和客户端都会缓存元数据。
客户端和Chunk服务器都不缓存文件数据
大部分程序要么以流的方式读取一个巨大文件，要么工作集太大根本无法被缓存 → 客户端缓存文件数据意义不大。
Chunk以本地文件的方式保存， Linux操作系统的文件系统缓存会把经常访问的数据缓存在内存中。Chunk服务器不必特别缓存文件数据。
```

##### GFS读取流程

![1571972900106](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571972900106.png)

```
1.客户端把需要读取的文件名和程序指定的字节偏移，根据固定的 Chunk 大小，转换成文件的 Chunk 索引。
2.客户端把文件名和 Chunk 索引发送给 Master 节点。
3.Master节点将相应的Chunk标识和副本的位置信息发还给客户端。
4.客户端用文件名和 Chunk 索引作为 key 缓存这些信息。
5.客户端发送请求（包含Chunk标识和字节范围）到其中的一个副本Chunkserver，一般会选择最近的。
6.该Chunkserver读取chunk数据返回客户端

在开始对Chunk的数据读取操作后，客户端不必再和Master节点通讯，除非缓存的元数据信息过期或者文件被重新打开。

客户端通常会在一次请求中查询多个Chunk信息，Master节点的回应也可能包含了被请求Chunk的后续Chunk的信息——实际过程中，客户端通常会在一次请求中查询多个Chunk信息。
    ↓
    同样代价额外元数据，避免客户端和Master节点未来可能会发生的几次通讯。

Master节点可通过全局的信息精确定位Chunk的位置并制定复制决策。
单一Master节点的策略 → 简化了系统设计。
	↓
	为避免Master节点成为系统的瓶颈，必须减少对Master节点的读写：
1）客户端不通过Master节点读写文件数据，仅向Master节点询问它应该联系的Chunk服务器；
2）客户端将这些元数据信息（及相邻元数据）缓存一段时间；
3）后续的数据读写操作直接对Chunk服务器进行。

Chunk分配及其大小
惰性分配策略：每个Chunk（副本）都以普通Linux文件的形式保存在Chunk服务器上，只在需要的时候才扩大。
     ↓ 
避免因内部碎片造成的空间浪费。
Chunk的大小是关键的设计参数之一（设置为64MB这个远远大于一般文件系统的块尺寸存在着争议）。

Chunk过热问题
存放某个可执行文件的几个Chunk服务器被数百个客户端的并发访问请求导致系统局部过载。
解决方法：
1）使用更大的复制参数来保存可执行文件；
2）错开批处理队列系统程序的启动时间；
3）允许客户端从其它客户端读取数据。
```

![1571988382544](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988382544.png)

![image-20200228175956151](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200228175956151.png)

##### GFS写入流程

```
GFS必须将对数据块的修改同步到每一个副本。
多个应用可能同时修改同一数据块
    	↓
并发应用下为使多个副本之间保持一致，需要为修改操作定义统一的时序。
问题：为减少master负担，系统采用了元数据缓存机制，client在获得并缓存副本位置后就不再和master交互，谁来定义时序
		↓   
需要有人代替master的角色
        ↓   
选出一个master的代理来完成时序约定任务。
        ↓
租约（lease）机制
master建立租约，并将其授权给某个副本，称其为primary，由它来确定数据修改的顺序，其它副本照做。
```

![1571988973437](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988973437.png)

```
1)客户机向Master节点询问哪个Chunk服务器持有当前租约，以及其它副本的位置。如果没有Chunk持有租约，Master就选择其中一个副本建立一个租约。

2)Master将主Chunk的标识符以及其它副本（secondary副本/二级副本）的位置返回给客户机。客户机缓存这些数据以便后续操作。此后只有在主Chunk不可用或主Chunk回复信息表明它已不再持有租约时，客户机才重新联系Master节点。

3）客户机把数据推送到所有的副本上（可按任意的顺序推送）。Chunk服务器接收到数据并保存在它的内部LRU缓存中，直到数据被使用或过期交换出去。

4）当所有的副本都确认收到了数据后，客户机发送写请求到主Chunk服务器。主Chunk为接收到的所有操作分配连续的序列号（这些操作可能来自不同的客户机，序列号保证了操作顺序执行），按序列号的顺序把操作应用到primary自己的本地状态中。

5）主Chunk把写请求传递到所有的二级副本。每个二级副本依照主Chunk分配的序列号以相同的顺序执行这些操作。

6）所有的二级副本回复主Chunk“它们已经完成了操作”。

7）主Chunk服务器回复客户机（任何副本产生的任何错误都会返回给客户机）。出现错误时，写操作可能在主Chunk和一些二级副本执行成功。（如果主Chunk上操作失败，操作不会被分配序列号，也不会传递）客户端请求被确认为失败，被修改的region处于不一致状态。
客户机代码通过重复执行失败的操作来处理这样的错误。在从头开始重复执行之前，客户机会先尝试几次步骤(3)~(7)。
```

```
写数据分割
应用程序一次写入的数据量较大，或者数据跨越了多个Chunk。
         ↓
客户机代码会把它们分成多个写操作。
    这些操作都遵循上述控制流程，但可能会被其它客户机上同时进行的操作打断或者覆盖。
         ↓
    共享的文件region的尾部可能包含来自不同客户机的数据片段。
    写操作的流程使得这些分解后的写操作在所有副本上都以相同的顺序执行完成，实现Chunk副本的一致性。

记录追加（原子性操作）
记录追加：GFS提供的特殊的修改操作，一种原子性的数据追加操作，除了在主Chunk有些额外的控制逻辑，遵循一般的写操作控制流程：
1）客户机把数据推送给文件最后一个Chunk的所有副本。
2）发送请求给主Chunk。
3）主Chunk检查这次记录追加操作是否会使Chunk超过最大尺寸（64MB）。如果超过则主Chunk先将当前Chunk填充到最大尺寸，之后通知所有二级副本做同样的操作，然后回复客户机要求其对下一个Chunk重新进行记录追加操作。

记录追加：客户机只需指定要写入的数据，GFS来实现至少有一次原子的写入操作成功执行（写入一个顺序的字节流），写入的数据追加到GFS指定的偏移位置上，之后GFS返回这个偏移量给客户机。
如果记录追加操作在任何一个副本上失败了，客户端会重试追加操作。
	↓重新记录追加
一个Chunk的不同副本可能包含不同的数据（重复包含一个记录全部或者部分的数据）。
GFS并不保证Chunk的所有副本在字节级别是完全一致的，只保证数据作为一个整体原子的被至少写入一次。如图：

记录追加原子性的技术保证
      记录追加用O_APPEND打开文件，是一个原子操作：移动到末端，写数据。
记录追加O_APPEND打开文件提供了无锁的文件追加方式，中间的插入是无效的。
```

![image-20200319150108324](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319150108324.png)

##### GFS一致性模型

```
GFS 支持一个宽松的一致性模型，这个模型能够很好的支撑我们的高度分布的应用，同时还保持了相对
简单且容易实现的优点。

文件命名空间的修改（如文件创建）是原子的
    ↓（一致性保证机制）
仅由Master节点控制，命名空间锁提供原子性和正确性，Master节点的操作日志定义写操作在全局的顺序。
```

![image-20200319150741376](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319150741376.png)

```
一致性保障——客户端Chunk位置缓存刷新
缓存机制：Chunk位置信息会被客户端缓存
    ↓
失效窗口：在缓存的超时时间和文件下一次被打开的时间之间存在一个时间窗，在信息刷新前，客户端有可能从一个失效的副本读取了数据。   
缓存更新：文件再次被打开后会清除缓存中与该文件有关的所有Chunk位置信息。
    当一个Reader重新尝试并联络Master服务器时，它就会立刻得到最新的Chunk位置信息。
缓存失效的影响：
    文件大多数都只是进行追加操作
        ↓
    一个失效的副本通常返回一个早已结束的Chunk，而不是过期的数据。

有效性校验checksum
文件region的Checksum：Writers在每条写入的记录中都包含了额外的信息，如Checksum，用来验证它的有效性。
Readers可以利用Checksum识别、抛弃偶然性的填充数据和重复内容。

一致性保障——服务器检测与数据校验
组件的失效可能损坏或者删除数据。
数据损坏：通过Checksum校验数据是否损坏。
    发现问题则尽快利用有效的副本恢复数据。只有当一个Chunk的所有副本在GFS检测到错误并采取应对措施之前全部丢失，该Chunk才会不可逆转的丢失。
发现失效Chunk服务器：Master服务器通过和所有Chunk服务器的定期“握手”来找出失效的Chunk服务器。

一致性保障机制-操作顺序、版本检测
   经过了一系列成功修改操作后，GFS确保被修改的文件region已定义，并且包含最后一次修改操作写入的数据。
GFS确保上述已定义效果的措施包括：
	（a）对Chunk的所有副本的修改操作顺序一致；
	（b）使用Chunk版本号来检测副本是否因为它所在的Chunk服务器宕机而错过了修改操作而导致其失效。
失效副本处理：
    （1）失效副本位置信息Master服务器将不再返回客户端；
    （2）该副本不再进行任何修改操作；
    （3）该副本会被垃圾收集系统尽快回收
   
Chunk版本号增加：
    Master节点和Chunk签订一个新租约时
     ↓
    Master增加Chunk版本号，然后通知最新的副本。
Master和这些副本都把新的版本号记录在它们持久化存储的状态信息中。
该动作发生在任何客户机得到通知以前（也是对新版Chunk开始写之前）。

Chunk版本（时效）检测-保护措施：（Master）
1通知版本号）Master在通知客户机某个Chunk服务器持有租约、或者指示从哪个Chunk服务器进行克隆时，消息中都附带了Chunk的版本号。
2过期不回复）Master在回复客户机的Chunk信息请求时会认为那些过期的Chunk不存在（即使过期副本还未被回收）。
3例行清理）Master例行垃圾回收时移除所有的过期失效副本。

Chunk时效检测-版本号验证
Chunk服务器失效：Chunk副本可能因错失一些修改操作而过期失效。
         ↓
    Master保存每个Chunk的版本号以区分当前副本和过期副本。
客户机或者Chunk服务器：在执行操作时都会验证版本号以确保总是访问当前版本的数据。

Chunk版本变更异常情况
	如果某个副本所在的Chunk服务器处于失效状态，则该副本版本号不会增加。
      ↓
    该Chunk服务器重新启动并向Master报告其拥有的Chunk集合及版本号时， Master可检测出这些过期的Chunk。

    如果Master看到一个比它记录的版本号更高的版本号，Master会认为它和Chunk服务器签订租约的操作失败了，此时会选择更高的版本号作为当前的版本号。

```

![1571988445075](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988445075.png)

![1571988523815](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988523815.png)

![1571988580733](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988580733.png)

![1571988633029](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988633029.png)

![1571988680596](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571988680596.png)

##### GFS锁机制

```
读锁和读写锁：Master节点的每个操作在开始之前都要获得一系列的锁。
例：一个写操作涉及/d1/d2/…/dn/leaf，那么它首先要获得目录/d1，/d1/d2，…，/d1/d2/…/dn的读锁，以及/d1/d2/…/dn/leaf的读写锁。
注：leaf可以是一个文件，也可以是一个目录。 

锁的分配：
    因为名称空间可能有很多节点，读写锁采用惰性分配策略，并且在不再使用时立刻删除。
避免死锁：锁的获取依据一个全局一致的顺序，首先按名称空间的层次排序，在同一个层次内按字典顺序排序。

读写锁支持对同一目录的并行操作，可以在同一个目录下同时创建多个文件（每一个操作都获取一个目录名的上的读锁和文件名上的写入锁）。
目录名的读锁防止目录被删除、改名以及被快照。
文件名的写入锁串行化文件创建操作，确保不会多次创建同名的文件。
```

##### chunk创建/移动/回收

```
Chunk在Writer真正写入数据时才被物理上创建。
Master创建一个Chunk时，需要选择在哪里放置初始的空副本，考虑几个因素：
	平均硬盘使用率，Chunk副本分布在多个机架，每个Chunk服务器上创建次数
	
Chunk的有效副本数量少于用户指定的复制因数时（某些副本坏了，或者Chunk副本的复制因数提高了）
     ↓
Master会重新复制它。

Master选择优先级最高的Chunk，然后命令某个Chunk服务器直接从可用的副本”克隆”一个副本出来。

Master周期性地对副本负载均衡：检查当前副本分布情况，移动副本以更好的利用硬盘空间、更有效的进行负载均衡。

垃圾回收——惰性、日志、隐藏、孤立
惰性：
    GFS文件删除后不会立刻回收其物理空间，采用惰性回收策略，只在文件和Chunk级的常规垃圾收集时进行。

日志、隐藏：
    当文件被应用程序删除时，Master立刻把删除操作以日志的方式记录下来。但是，Master并不马上回收资源，而是把文件名改为一个包含删除时间戳的、隐藏的名字。
    
名称空间定期回收：
    Master会对文件系统命名空间做常规扫描，此时它会删除所有三天前的隐藏文件（时间间隔可设置）。直到文件被真正删除，它们仍旧可以用新的特殊名字读取，也可通过把隐藏文件改名为正常显示文件名的方式“反删除”。

孤立：
    当隐藏文件被从名称空间中删除，Master内存中保存的这个文件的相关元数据才会被删除。这也有效的切断了文件和它包含的所有Chunk的连接。

垃圾回收——删除孤儿Chunk
Master会对Chunk名字空间做类似的常规回收扫描，此时会找到孤儿Chunk（不被任何文件包含）并删除它们的元数据。
Chunk服务器在与Master交互的心跳信息中，报告它拥有的Chunk子集信息，Master节点回复这其中哪些Chunk在Master节点保存的元数据中已不存在。Chunk服务器可以任意删除这些Chunk的副本。
```

#### BigTable

```
Bigtable 是一个分布式结构化数据存储系统，用来处理海量数据：通常是分布在数千台普通服务器上PB级的数据。
Bigtable 支持单行上事务处理，用户可以对存储在一个行关键字下的数据进行原子性的读-更新-写操作。
设计思想
    简单数据模型
    稀疏、分布式、多维度排序Map（行关键字+列关键字+时间戳，不解析）
    列族
    Tablet、SStable、数据块
    行关键字下的原子性操作
体系架构
    分布式锁服务组件Chubby（存储BigTable的模式信息、自引导指令）
    Master（负载均衡、修改模式）
运行管理
    Root Tablet——METADATA——用户表Tablet（SSTable）
    Tablet服务器←→独占锁
Table服务
    Tablet变更（分割由Tablet服务器启动）
    更新操作的Redo日志，变更memtable
    基于日志的恢复-热点-日志排序（master协调）
    Minor Compaction（更新、Tablet迁移+2次）、
    Merging Compaction、
```

##### BigTable数据模型

```
Bigtable  是一个稀疏的、分布式的、持久化存储的多维度排序 Map 
Map的索引=行关键字+列关键字+时间戳。
Map中的每个 value 都是一个未经解析的 byte 数组。
(row:string, column:string,time:int64)->string
存储的数据都视为字符串，Bigtable本身不去解析这些字符串。
          ↓
客户程序可以把各种结构化或者半结构化的数据串行化（序列化）到这些字符串里。
```

![image-20200319172856638](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319172856638.png)

**行**

```
每一行可参与动态分区，一个分区叫做一个”Tablet”。
Tablet是数据分布和负载均衡调整的最小单位。（SSTable是列存储的单位）
这样做的结果是，当操作只读取行中很少几列的数据时效率很高，通常只需要很少几次机器间的通信即可完成。
表中的行关键字可以是任意的字符串（目前支持最大 64KB 的字符串，但是对大多数用户，10-100 个字节就足够了）。
对同一个行关键字的读或者写操作都是原子的（不管读或者写这一行里多少个不同列）
Bigtable 通过行关键字的字典顺序来组织数据。
```

**列族**

```
列关键字组成的集合叫做“列族“，列族是访问控制的基本单位。
存放在同一列族下的所有数据通常都属于同一个类型（可压缩列族）。
列族在使用之前必须先创建，然后才能在列族中任何的列关键字下存放数据；
一张表的列族不能太多（最多几百个），并且列族在运行期间很少改变。
访问控制、磁盘和内存的使用统计都是在列族层面进行的。
	允许一些应用添加新基本数据、一些应用可以读取基本数据并创建继承的列族、一些应用则只允许浏览数据
列关键字的命名语法如下：列族：限定词。 列族的名字必须是可打印的字符串，而限定词的名字可以是任意的字符串。
    1）Webtable的language列族，存放撰写网页的语言，只有一个列关键字，用来存放网页的语言标识ID。
    2）anchor列族，限定词是引用该网页的站点名，每个列关键字代表一个锚链接，每列的数据项存放链接文本。
```

**时间戳**

```
每个数据项可包含同一份数据的不同版本，通过时间戳（int64）来索引。
时间戳赋值：
    1）通过Bigtable（可表示精确到毫秒的“实时”时间）
    2）由应用程序自己生成具有唯一性的时间戳。
数据项中不同版本的数据按照时间戳倒序排序（最新的数据排在最前面）。    
```

##### 基本操作

```
Bigtable提供的API函数，至少包括： 
1）建立和删除表；
2）建立和删除列族。

客户程序可以写入或者删除表中的值，如：
1）增加、删除锚点（增加列）；
2）从每个行中查找值；
3）遍历表中的一个数据子集。

Bigtable支持单行上的事务处理，用户可以对存储在一个行关键字下的数据进行原子性的读写操作。

BigTable还提供修改集群、表和列族的元数据的API（如修改访问权限）。

Bigtable可以和MapReduce（Google开发的大规模并行计算框架）一起使用：
    已经开发了一些Wrapper类，Bigtable可以作为MapReduce框架的输入和输出。
```

##### 体系架构

```
Bigtable建立在其它几个Google基础构件上：
1）使用Google的分布式文件系统(GFS)存储日志文件和数据文件；
2）依赖高可用的、序列化的分布式锁服务组件Chubby
3) BigTable 集群通常运行在一个共享的机器池中，池中的机器还会运行其它的各种各样的分布式应用程序
4) BigTable 依赖集群管理系统来调度任务、管理共享的机器上的资源、处理机器的故障、以及监视机器的状态。
```

![image-20200319174638888](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319174638888.png)

```
Bigtable 包括了三个主要的组件：
链接到客户程序中的库、一个 Master 服务器和多个 Tablet 服务器。
针对系统工作负载的变化情况，BigTable 可以动态的向集群中添加（或者删除）Tablet 服务器。
```

##### Chubby

```
Chubby：高可用的、序列化的分布式锁服务组件，一个Chubby服务包括了5个活动的副本，其中一个副本被选为Master并处理请求。

Bigtable 使用 Chubby 完成以下的几个任务：
如果Chubby长时间无法访问，BigTable就会失效。
    确保在任何给定的时间内最多只有一个活动的 Master 副本；
    存储 BigTable 数据的自引导指令的位置；
    查找 Tablet 服务器，以及在 Tablet 服务器失效时进行善后；
    存储 BigTable 的模式信息（每张表的列族信息）；
    以及存储访问控制列表。
    
Chubby服务可用的前提：
    1）大多数副本都正常运行；
    2）彼此之间能够互相通信。
当有副本失效的时候，Chubby使用算法来保证副本的一致性。

Chubby有一个名字空间（内容包括目录和小文件）。
    每个目录或文件可以看作一个锁，读写文件的操作都是原子性的。

Chubby客户程序一致性缓存（客户端程序库提供）
    每个Chubby客户程序都维护一个与Chubby服务的会话（session）并持有租约。

会话失效（及时续签）：如果客户程序不能在租约到期的时间内重新签订会话的租约，则该会话过期失效。
          ↓
会话失效则客户程序拥有的相关信息失效：
    锁、打开的文件句柄。
    
Chubby客户程序可以在文件和目录上注册回调函数。
          ↓
当文件或目录改变或者会话过期时，回调函数会通知客户程序。
```

##### Master

```
Master服务器主要负责以下工作：实际Master服务器的负载是很轻的
    1）为Tablet服务器分配Tablets
    2）检测新加入的或者过期失效的Table服务器
    3）对Tablet服务器进行负载均衡
    4）对保存在GFS上的文件进行垃圾收集
    5）处理对模式的相关修改操作（例如建立表和列族）
```

##### Tablet服务器

```
每个Tablet服务器都管理一个Tablet的集合（通常数十个至上千个Tablet）：
    1）处理它加载的Tablet的读写操作；
    2）在Tablet过大时对其分割。

和很多Single-Master类型的分布式存储系统类似，
  客户程序读取数据不经过Master，直接和Tablet服务器通信进行读写操作。
          ↓
  客户程序不必通过Master来获取Tablet的位置信息，大多数客户程序甚至完全不需要和Master服务器通信，导致了在实际应用中Master服务器的负载很轻。
```

##### Tablet

```
一个BigTable集群存储了很多表，每个表包含一个Tablet的集合
       每个Tablet包含某个范围内的行的所有相关数据。
      （ → SSTable → 数据块）

初始状态下，一个表只有一个Tablet，随着表中数据的增长，它被自动分割成多个Tablet。
缺省情况下，每个Tablet的尺寸大约是100MB到200MB。
```

##### 数据文件SSTable

```
BigTable内部存储数据的文件：Google SSTable。
    SSTable是一个持久化的、排序的、不可更改的Map结构（key-value映射），key和value的值都是任意的字节串。

查询：可以对SSTable查询与一个key值相关的value，或者遍历某个key值范围内的所有的key-value对。

从内部看，SSTable是一系列的数据块，通常每个块的大小是64KB（可以配置）。

数据块的定位：使用块索引（通常存储在SSTable的末尾）定位数据块。
块索引缓存：在打开SSTable时块索引被加载到内存。
    也可以选择把整个SSTable都放在内存中。
```

##### Tablet三层寻址

Tablet的位置信息：使用一个三层的、类似Ｂ+树的结构存储。

![image-20200319181126350](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319181126350.png)

```
Tablet层次结构:
	Chubby file一个存储在Chubby中的文件，包含Root Tablet的位置信息。
    第一层：Root Tablet，包含了一个特殊的METADATA表，记录METADATA里所有的Tablet的位置信息。
    第二层：METADATA表，其中的每个Tablet包含了一个用户Tablet的集合。
    第三层：用户表tablet。
Root Tablet：
    1）实际上是METADATA表的第一个Tablet；
    2）永远不会被分割 ，从而保证Tablet的位置信息存储结构不会超过三层。

METADATA表中，每个Tablet的位置信息都存放在一个行关键字下
     行关键字=Tablet所在的表的标识符⊕Tablet的最后一行编码
METADATA的每一行存储约1KB的内存数据。
	这种三层结构的存储模式下，容量为128MB的METADATA Tablet，对应2^17行，两层可标识2^34个Tablet的地址。如果每个Tablet存储128MB数据，那么一共可以存储2^61字节数据。
METADATA表还存储次级信息(secondary information)
主要包括每个Tablet的事件日志（例如一个服务器何时开始为该Tablet提供服务）。
          ↓
有助于排查错误和性能分析。

SSTable注册信息:
    SSTable注册：每个Tablet的SSTable都在METADATA表中注册。
    SSTable删除：Master服务器采用“标记-删除”的垃圾回收方式删除废弃的SSTable。
    Root SSTable集合的保存：METADATA表保存。
```

**Tablet的客户端缓存**

```
客户程序使用的库：缓存Tablet的位置信息，Tablet 的地址信息是存放在内存里,对它的操作不必访问GFS文件系统。

预取Tablet地址：每次需要从METADATA表中读取一个Tablet的元数据时，都会多读取几个Tablet的元数据。
          ↓
进一步的减少访问的开销。

客户端可能未缓存某个Tablet的地址信息（例如缓存为空），或发现缓存的地址信息不正确（过期）。
    ↓
客户程序在树状的存储结构中递归的查询Tablet位置信息。
```

##### Tablet分配

```
一个Tablet只能分配给一个Tablet服务器。
活跃及分配情况：Master服务器记录当前有哪些活跃的Tablet服务器以及Tablet的已分配和未分配情况。

某个Tablet没有被分配、并且有一个Tablet服务器有足够的空闲空间装载该Tablet
          ↓
Master向该Tablet服务器发送一个装载请求，把Tablet分配给该服务器。
```

**Tablet服务器状态：独占锁监控**

```
使用Chubby跟踪记录Tablet服务器的状态。
    Tablet服务器启动时在Chubby的一个指定目录（服务器目录）下建立一个有唯一性名字的文件，并且获取该文件的独占锁。Master实时监控该目录，因此 Master 服务器能够知道有新的 Tablet 服务器加入
Chubby提供的机制使Tablet服务器能够知道自己是否还持有锁

Tablet服务器丢失了Chubby上的独占锁（比如网络断开导致Tablet服务器和Chubby的会话丢失）
	↓
停止对Tablet提供服务

Chubby文件还存在：Tablet服务器试图重新获得对该文件的独占锁。
Chubby文件不存在：Tablet服务器就不能再提供服务，它自行退出（it kills itself）。

Tablet服务器终止（例如运行该Tablet服务器的主机从集群中移除）
        ↓ 
尝试释放它持有的文件锁，Master服务器会尽快把该节点的Tablet分配到其它Tablet服务器。
```

**Tablet服务器状态检测**

```
Master服务器轮询Tablet服务器文件锁的状态
——检测何时Tablet服务器不再为Tablet服务。

某个Tablet服务器报告丢失了自己的文件锁，或者Master最近几次尝试和它通信都没有得到响应。
        ↓
Master尝试获取该Tablet服务器文件的独占锁。
        ↓
若Master服务器成功获取了独占锁
        ↓
Chubby运行正常。
        ↓
Tablet服务器宕机，或者不能和Chubby通信。
        ↓
1）Master删除该Tablet服务器在Chubby上的服务器文件，以确保它不再给Tablet提供服务；
2）把之前分配给它的所有Tablet放入未分配的Tablet集合中。

若Master故障
Master的Chubby会话过期  → 主动退出。
          ↓
确保Bigtable集群在Master和Chubby之间网络故障时仍然可以使用。
Master故障不会改变现有Tablet在Tablet服务器上的分配状态
字典数据在Root Tablet上，通过Chubby获得
如Chubby长时间无法访问，则BigTable失效。
```

当集群管理系统启动了一个 Master 服务器之后，Master 服务器首先要了解当前 Tablet 的分配状态，之后才能够修改分配状态。Master 服务器在启动的时候执行以下步骤：

```
（1）从Chubby获取一个唯一的Master锁，用来阻止创建其它的Master服务器实例；
（2）扫描Chubby的服务器文件锁存储目录，获取当前正在运行的服务器列表；
（3）和所有的正在运行的Tablet表服务器通信，获取每个Tablet服务器上Tablet的分配信息；
（4）扫描METADATA表获取所有的Tablet的集合，在扫描过程中如果发现还没有分配的Tablet（节点退出、故障、Tablet分裂），就将其加入未分配的Tablet集合等待合适的时机分配。
```

##### Tablet集合变更

```
Tablet集合的改变：
    1）建立一个新表；
    2）删除一个旧表；
    3）两个Tablet合并；
    4）一个Tablet被分割成两个小的Tablet。
Master跟踪记录所有这些事件。
1）~3）由master启动，4）由tablet服务器启动。

Tablet服务器启动分割事件，完成后，需在METADATA表中记录新的Tablet的信息，以示提交该操作。
 		↓
分割操作提交后，Tablet服务器通知Master已提交信息。

若信息没有通知到Master
        ↓
Master在要求Tablet服务器装载（已被分割的）子表时会发现一个新的Tablet。
通过对比METADATA表中Tablet的信息，Tablet服务器会发现Master要求其装载的Tablet不完整，Tablet服务器重新向Master服务器发送通知信息。
```

##### Tablet服务：更新日志

```
Tablet 的持久化状态信息保存在 GFS 上。更新操作提交到 REDO 日志中 。
在更新操作中，最近提交那些存放在一个排序的缓存（memtable）；较早的更新存放在一系列SSTable 中。
```

![1571990342471](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571990342471.png)

##### Tablet服务：保存数据

```
BigTable 内部存储数据的文件是 Google SSTable 格式的。
SSTable 是一个持久化的、排序的、不可更改的Map 结构，而 Map 是一个 key-value 映射的数据结构，key 和 value 的值都是任意的 Byte 串。
可以对 SSTable进行如下的操作：查询与一个 key 值相关的 value，或者遍历某个 key 值范围内的所有的 key-value 对。
从内部看SSTable 是一系列的数据块（通常每个块的大小是 64KB，这个大小是可以配置的）。
SSTable 使用块索引（通常存储在SSTable 的最后）来定位数据块；在打开 SSTable 的时候，索引被加载到内存。每次查找都可以通过一次磁盘搜索完成：首先使用二分查找法在内存中的索引里找到数据块的位置，然后再从硬盘读取相应的数据块。
也可以选择把整个 SSTable 都放在内存中，这样就不必访问硬盘了。读写示例如下图：
```

![1571989399915](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989399915.png)

##### Tablet服务：恢复

```
1）Tablet服务器先从METADATA表中读取它的元数据（包含组成这个Tablet的SSTable列表以及一系列的Redo Point，这些Redo Point指向可能含有该Tablet数据的已提交的日志记录）。
2）Tablet服务器把SSTable的索引读进内存。
3）Tablet服务器通过重复Redo Point之后提交的更新来重建memtable。
```

![1571989599804](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989599804.png)

##### Tablet服务：写操作

```
Tablet服务器进行写操作时先检查：
    操作格式是否正确、
    操作发起者是否有执行权限（从一个Chubby文件中读取具有写权限的操作者列表（一般在Chubby客户缓存里），从而验证权限。）。

当写操作提交后，写的内容插入memtable。成功的修改操作会记录在提交日志里。

批量提交方式（group commit）：提高包含大量小的修改操作的应用程序吞吐量。
```

![1571989498341](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989498341.png)

##### Tablet服务：Minor Compaction

```
写操作使memtable大小不断增加
          ↓
memtable大小到达门限值
          ↓
Minor Compaction：该memtable被冻结，创建一个新的memtable。被冻结住memtable会被转换成SSTable，然后写入GFS。当进行 Tablet 的合并和分割时，正在进行的读写操作能够继续进行

目的：
1）收缩Tablet服务器使用的内存；
2）在服务器灾难恢复过程中减少必须从提交日志里读取的数据量。

定期在后台执行Merging Compaction：读取一些SSTable和memtable的内容，”合并“成一个新的SSTable ，限制这类文件的数量。
    Merging Compaction过程完成后可删除输入的这些SSTable和memtable。
```

![1571989538135](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989538135.png)

![1571989599804](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989599804.png)

##### Tablet服务：读操作

```
Tablet服务器进行读操作时会作类似的完整性和权限检查。
读操作在一个由一系列SSTable和memtable合并的视图里执行
    ↑
    SSTable和memtable是按字典排序的数据结构，因此可高效生成合并视图。

当进行Tablet的合并和分割时，正在进行的读写操作能够继续进行。

```

![1571989688701](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989688701.png)

##### 读优化：局部性群组

```
客户程序可以将多个列族组合成一个局部性群组，对应一个单独的SSTable。
将通常不会一起访问的列族分割成不同的局部性群组
          ↓
提高读操作的效率。
    如，在 Webtable表中，网页的元数据（比如语言和 Checksum）可以在一个局部性群组中，网页的内容可以在另外一个群组：当一个应用程序要读取网页的元数据的时候，它没有必要去读取所有的页面内容。
    
可以以局部性群组为单位设定一些调试参数（例如设定一个局部性群组全部存储在内存中，从而优化需要频繁访问的小块数据。Bigtable内部利用这个特性提高METADATA表中具有位置相关性的列族的访问速度。）
```

##### 读优化：Tablet服务器缓存

```
Tablet服务器使用二级缓存策略：
    第一级缓存——扫描缓存，主要缓存Tablet服务器通过SSTable接口获取的Key-Value对。
    第二级缓存——Block缓存，缓存从GFS读取的SSTable的Block。
```

![1571989767781](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989767781.png)

##### 读优化：Bloom过滤器

客户程序可以对特定局部性群组的SSTable指定Bloom过滤器缓存来减少硬盘访问的次数：

使用Bloom过滤器查询一个SSTable是否包含了特定行和列的数据

![1571989858092](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571989858092.png)

```
Bloom filter（布隆过滤器）被用来检测一个元素是不是集合中的一个成员，它具有很好的空间和时间效率
Bloom filter检测结果：有100%的召回率
   是 → 该元素不一定在集合中；
   否 → 该元素一定不在集合中。
Bloom filter采用的是哈希函数的方法：将一个元素映射到一个m长度的阵列上的一个点，当这个点是 1 时，那么这个元素在集合内，反之则不在集合内。
使用k个哈希函数对应k个点，如果所有点都是 1，则元素可能在集合中，如果有0，则元素不在集合中。
y1不是集合中的元素，y2属于这个集合或者是一个false positive。
```

![image-20200319212007212](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319212007212.png)

##### Commit日志实现

```
如果把对每个Tablet的操作的Commit日志都存一个文件，则会产生大量的日志文件及其并行写操作
    ↓
1）导致大量的磁盘Seek操作；
2）影响批量提交的优化效果。

设置每个Tablet服务器一个Commit日志文件，混合对多个Tablet的修改日志记录，以追加方式写入同一个日志文件。
```

##### TabletServer宕机恢复

```
Tablet服务器宕机
        ↓
它加载的Tablet将被移到很多其它的Tablet服务器上（每个Tablet服务器都装载很少的几个宕机服务器的Tablet）。

要恢复一个Tablet的状态：新的Tablet服务器要从原Tablet服务器日志中提取记录并重新执行。

假如有100台Tablet服务器，每台都加载了失效的Tablet服务器上的一个Tablet，那么，这个日志文件就要被读取100次（每个服务器读取一次）。
一个批处理程序，被100个节点同时访问，产生系统的热点。
        ↓
把日志按照关键字（table，row name，Log Sequence Number）排序   →  对同一个Tablet的修改操作日志记录连续存放在一起
        ↓
恢复时只要一次磁盘Seek操作，之后只需顺序读取。

并行排序：先将日志分割成64MB的段，之后在不同的Tablet服务器对段进行并行排序。
Master服务器来协同处理该排序工作，在一个Tablet服务器表明自己需要从Commit日志文件恢复Tablet时开始执行。
```

##### Tablet迁移

```
Master将某个Tablet从一个Tablet服务器移到另外一个Tablet服务器时，源Tablet服务器会对这个Tablet做两次Minor Compaction：
第一次，减少Tablet服务器的日志文件中没有归并的日志记录，从而减少恢复的时间。Compaction完成后，源服务器停止为该Tablet提供服务。
第二次，在卸载Tablet之前，源Tablet服务器再做一次Minor Compaction，以消除刚才压缩过程中又产生的未归并的记录，在这之后Tablet就可以被装载到新的Tablet服务器上并且不需要从日志中进行恢复了。
```

#### Paxos算法

```
Paxos 算法：Lamport于1990年提出，是一种基于消息传递的一致性算法。
Paxos算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。
Paxos算法目的：解决分布式环境下一致性的问题。
     多个节点并发操纵数据，如何保证在读写过程中数据的一致性，要求解决方案能适应分布式环境下的不可靠性 。

Paxos算法通过多个监督者来增强可靠性，基本思想：
    1）通过监督者们的投票来表决数据的状态变化，
    2）保证所有对数据的访问都遵从这种表决。
    
算法变种：
    Classic Paxos——1个实例写入(确定1个值) 需要2轮RPC。
    Multi Paxos——约为1轮RPC，将多个paxos实例的第1轮合并，使得这些paxos只需运行第二轮RPC。
    Fast Paxos——没冲突时1轮RPC确定一个值，有冲突时2轮RPC确定一个值。

Paxos有效的基本保障：
    法定集合：将一个超过半数的集合称之为法定集合
    比如数字1、2、3、4、5，共5个元素，{1,2,3}有三个元素就是法定集合。
    法定集合性质：任意两个法定集合，必定存在一个公共的成员。

算法应用：
    Google的Chubby（Multi Paxos算法）、
    Hadoop中的ZooKeeper、
    MegaStore、Spanner等系统。

Paxos系统中的角色：
    Proposer: 提出提案 (Proposal)。Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)。
    Acceptor：参与决策，回应Proposers的提案。收到Proposal后可以接受提案，若Proposal获得多数Acceptors的接受，则称该Proposal被批准。
    Learner：不参与决策，从Proposers/Acceptors学习最新达成一致的提案（Value）

其他概念：
	instance(实例)：一次Paxos算法执行。
	proposal（议案）：经发起而未经批准的提案。
	value（决议）：被最终批准通过的议案中的value称为决议。

二段提交原则（2PC）：每个申请者在发送自己的提案之前，先检查有没有已经提议且被批准的值，如果有则放弃自己的提案，这样最终只有一个值被批准。

Paxos算法通过一个决议分为两个阶段（Learn阶段之前决议已经形成）：
    第一阶段：Prepare阶段。Proposer向Acceptors发出Prepare请求，Acceptors针对收到的Prepare请求进行Promise承诺。
    第二阶段：Accept阶段。Proposer收到多数Acceptors承诺的Promise后，向Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理。
    第三阶段：Learn阶段。Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners。

算法约束条件：
	1）一个acceptor必须接受他收到的第一个提案。
	2）提出一个编号为n具有值c的提案的前提是
        存在一个多数派，要么他们中没有人批准过编号小于n的任何提案，
        要么他们批准的提案中编号小于n的最大编号的提案值是c。
```

![image-20200319213608415](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319213608415.png)

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/2019062011162729.png)

```
整个paxos算法基本上就是围绕着proposal编号在进行，proposal的编号对算法至关重要：
	proposer忙于选择更大的编号提交proposal；    
	acceptor比较提交的proposal编号是否最大，只要编号确定了，所对应的value也就确定了。

Paxos议案ID生成算法：
Google的Chubby论文给出的解决方法：假设有n个proposer，每个节点编号为ir(0≤ir<n)，proposal编号的任何值s都应该大于它已知的最大值，并且满足：
      s %n = ir   （即s = m*n + ir）
proposer已知的最大值来自两部分：
   （1）proposer自己对编号自增后的值；
   （2）接收到acceptor的拒绝后所得到的值。
   
    例：3个proposer P1、P2、P3，开始m=0，编号分别为0、1、2。
    1）P1提交时发现P2已经提交，P2编号为1（>P1的编号0），因此P1重新计算编号：new P1 = 1*3+0 = 3；
    2）P3以编号2提交，发现小于P1的编号3，因此P3重新编号：new P3 = 1*3+2 = 5。

Paxos编号的活锁问题：
活锁：如果2个proposer都发现自己的编号过低转而提出更高编号的proposal，会导致死循环，这种情况也称为活锁。
“你编号高，我再比你更高，反复如此，算法永远无法结束。“

Leader选举
Lamport给出的解决活锁的办法是选举出一个proposer作Leader，所有的proposal都通过Leader来提交，当Leader宕机时马上再选举其他的Leader。
    Leader通过控制提交的进度来解决该问题：如果之前的proposal没有结果，之后的proposal就稍微等待，而不是急于提高编号再次提交。
     ↓
    Leader的出现相当于把一个分布式问题转化为一个单点问题，而单点的正确性和健壮性由选举机制保证。

Paxos容错性
	异常情况
      在算法执行的过程中会产生很多的异常情况：   
           proposer宕机，
            acceptor在接收proposal后宕机，
            proposer接收消息后宕机，
            acceptor在accept后宕机，
            learn宕机，
            存储失败，等等。
     持久存储
        为保证paxos算法的正确性，proposer、acceptor、learner都实现持久存储，以做到server恢复后仍能正确参与paxos处理。
```

##### CAP定理

```
分布式系统中有三种衡量特性：
一致性（Consistency）：每一个更新成功后，分布式系统中的所有节点都能读到最新的信息，即访问所有节点相当于同一份内容。这样的系统被认为是强一致性的。
可用性（Availability）：每一个请求都能得到响应。请求只需要在一定时间内返回即可，结果可以是成功或者失败，也不需要确保返回的是最新版本的信息。
分区容错性（Partition tolerance）：在网络中断、消息丢失的情况下，系统照样能够工作。网络分区是指由于某种原因网络被分成若干个孤立的区域，区域之间互不相通。

一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。
```

![image-20200319220312563](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319220312563.png)

##### 数据一致性模型

```
CAP理论 → 分布式系统如果不想牺牲一致性，则可能要牺牲可用性。
    ↓
数据一致性模型主要有以下几种：
   弱一致性[异步冗余]、
   最终一致性（一段时间达到一致性） [异步冗余] 、
   强一致性[同步冗余] 。

最终一致性的经典实现：Paxos算法（解决基于CAP原理构建的分布式系统环境中如何高效的达到数据的最终一致性的问题。）
强一致性的经典实现：Raft算法
```

#### Raft算法（）

```
分布式系统中的一致性问题(consensus problem)：对于一组服务器，给定一组操作，我们需要一个协议使得最后它们的结果（状态）达成一致。
在一个分布式系统中，因为各种意外可能，有的服务器可能会崩溃或变得不可靠，从而不能和其他服务器达成一致状态。
    ↓
需要一种Consensus协议确保容错性：即使系统中有一两个服务器宕机，也不会影响其处理过程。


```

#### MapReduce

```
MapReduce：一种线性可伸缩的编程模型，处理和生成超大数据集的算法模型。
受启发于Lisp和许多其他函数式语言的Map和Reduce的原语。
MapReduce架构的程序能够在大量的普通配置的计算机上实现并行化处理
（典型的MapReduce计算往往由几千台机器组成、处理TB级的数据）。

用户只表述要执行的简单运算，不必关心并行、容错、数据分布、负载均衡等复杂细节(这些问题都被封装在库里)。
表述过程：
1）创建一个Map函数处理一个基于key1/value 对的数据集合，输出中间数据集合（该集合再次以key2/value对的形式呈现）；
2）创建一个Reduce函数来合并处理中间数据内具有相同key2值的value（value list）。
```

![1571991040356](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571991040356.png)

![1571991244030](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/1571991244030.png)

```
Map调用被分布到多台机器上执行：Map调用的输入数据自动分割为M个数据片段的集合，这些数据片段在不同的机器上并行处理。

Reduce调用也分布在多台机器上执行：使用分区函数将Map调用产生的中间key值分成R个不同分区（例如hash(key) mod R），分区数量（R）和分区函数由用户来指定。
```

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/20151030095843101.png)

```
1.用户程序调用的MapReduce库首先将输入文件分成M个数据片度，每个数据片段的大小一般从 16MB到64MB(可以通过可选的参数来控制每个数据片段的大小)。然后master在机群中创建大量的用户程序副本（It then starts up many copies of the program on a cluster of machines，即，把map/reduce函数给不同的机器执行）。
2.这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是worker程序，由master分配任务。有M个Map任务和R个Reduce任务将被分配，master将一个Map任务或Reduce任务分配给一个空闲的worker。
3.被分配了map任务的worker程序读取相关的输入数据片段，从输入的数据片段中解析出key/value pair，然后把key/value pair传递给用户自定义的Map函数，由Map函数生成并输出的中间key/value pair，并缓存在本机内存中。
4.缓存中的key/value pair通过分区函数分成R个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair在本地磁盘上的存储位置将被回传给master，由master负责把这些存储位置再传送给Reduce worker（图中没画出来）。
5.当Reduce worker程序接收到master程序发来的数据存储位置信息后，Reduce worker使用remote procedure calls从Map worker所在主机的磁盘上读取这些缓存数据。当Reduce worker读取了（他所需要的）所有的中间数据后，通过对key进行排序后使得具有相同key值的数据聚合在一起。由于许多不同的key值会映射到相同的Reduce任务上，因此必须进行排序（从而保证相同的key的序列依次出现）。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。
6.Reduce worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce worker程序将这个key值和它相关的中间value值的集合传递给用户自定义的Reduce函数并计算出最终输出。Reduce函数的输出被追加到所属分区的输出文件。
7.当所有的Map和Reduce任务都完成之后，master唤醒用户程序。在这个时候，在用户程序里的对MapReduce调用才返回。

在成功完成任务之后，MapReduce的输出存放在R个输出文件中（对应每个Reduce任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这R个输出文件合并成一个文件–他们经常把这些文件作为另外一个MapReduce的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用
```

##### 关系代数

```
关系代数是一种过程化查询语言。包括一个运算集合，这些运算以一个或两个关系为输入，产生一个新的关系作为结果。
五个基本操作：
  集合并(∪)、集合差(-)、笛卡尔积(×)、选择(σ)、投影(π)、更名(ρ)
四个组合操作：
  集合交(∩)、联接(等值联接)、自然联接(RS ⋈ )、除法(÷) 、赋值（←）
```

##### 大数据中的迭代算法处理——Haloop

```
Haloop（迭代式MapReduce）
一些数据分析技术需要迭代计算（例如：PageRank、基于超文本的话题搜索、社会网络分析等等）
而MapReduce框架对此支持度不够（只能多次发起MapReduce任务）。
具有迭代结构的算法可用如下公式描述：
　Ri +1 =R0 ∪ (Ri  ⋈   L)
其中, R0表示初始化时的结果, L 表示一种不变的关系。
    当到达某检查点条件时（如结果收敛）迭代终止运行。
	例：K-mean聚类、PageRank。

重复多次执行MapReduce任务的缺点：
1）每次执行都需要重新装载数据、重新处理，而迭代过程中数据可分为动态数据和静态不变的数据两类。
2）迭代终止条件可能需要判断是否到达一个稳定点，该判断过程可能本身又需要在每次迭代执行过程中增加一次额外的MapReduce计算。
→任务调度开销、从磁盘读数据的开销、网络传输数据的开销。
```

**问题实例（PageRank算法）**

![image-20200319222231269](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319222231269.png)

**问题实例（多跳邻居）**

![image-20200319222525357](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319222525357.png)

```
优化思路：
要使得迭代效率提高，必须把循环体控制在job内，使之能运行多个map-reduce对，避免重复多次启动。
实现map/reduce静态数据缓存（内存或者本地磁盘），减少task的访问开销，须要task调度的配合以实现计算的本地化。
迭代的终止条件判断应该准确、高效。
```

##### Haloop系统框架

![image-20200319222935049](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-10-29-%E7%8E%B0%E4%BB%A3%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20200319222935049.png)

