---
layout: post
title: "吴恩达机器学习学习笔记汇总1"
date: 2019-07-24 10:36
toc: true
comments: true
categories: 技术学习
tags: 
	- 机器学习
---

#### 概论

##### **机器学习**

一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。

##### **监督学习**

我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测。主要有回归问题，即推出一个连续的输出；分类问题，其目标是推出一组离散的结果。

<!--more-->

##### **无监督学习**

没有任何的标签或者是有相同的标签或者就是没标签。它是学习策略，交给算法大量的数据，并让算法为我们从数据中找出某种结构。就是说你要自动地聚类那些个体到各个类

#### 线性回归（Linear Regression）

##### **代价函数**

也被称作平方误差函数，有时也被称为平方误差代价函数。

我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

![1563951973897](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563951973897.png)



##### **梯度下降**

是一个用来求函数最小值的算法，自动地找出能使代价函数J最小化的参数值。思想是：开始时我们随机选择一个参数的组合(θ0 ,θ1 ,......,θn )，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合，持续这么做直到到到一个局部最小值（local minimum）。

![1563952706310](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563952706310.png)

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20190201164155586.png)

##### **特征缩放**

将所有特征的尺度都尽量缩放到-1 到 1 之间

##### **正规方程法**

只要特征变量的数目并不大，标准方程是一个很好的计算参数𝜄的替代方法。

![1563957620861](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563957620861.png)



#### 逻辑回归（Logistic Regression）

##### **逻辑回归算法**

逻辑回归算法是一种分类算法，它输出变量范围始终在 0 和 1 之间，们将它作为分类算法使用。

![1563959367003](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563959367003.png)

![1563963025200](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563963025200.png)

![1567135303089](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1567135303089.png)

![1563963222472](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563963222472.png)

梯度下降，上面求导得到

![1563963605441](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563963605441.png)

除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有： **共轭梯度**（Conjugate Gradient）， **局部优化法**(Broyden fletcher goldfarb shann,BFGS)和 **有限内存局部优化法**(LBFGS) 



##### **过拟合(over-fitting)**

学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为 0），但是可能会不能推广到新的数据。

![1563967374118](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563967374118.png)

丢弃一些不能帮助我们正确预测的特征或者正则化。 保留所有的特征，但是减少参数的大小（magnitude）

##### **正则化**

正则化防止过拟合，对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度

![1563967773101](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1563967773101.png)



#### 神经网络(Neural Networks: Representation)

下图为一个 3 层的神经网络，第一层成为**输入层**（Input Layer），最后一层称为**输出层**（Output Layer），中间一层成为**隐藏层**（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）：

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-8369138f7b5e81e83f99b35067e03e7b_hd.jpg)

ai(j) 表示第j层的第i个激活单元，θ(j)表示从第j层到第j+1层时的权重矩阵，其尺寸为：以第 j+1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如：θ(1)的尺寸为3*4。g(x)为Sigmoid函数，hθ(X)就是最后的输出值。对于上图所示的模型，激活单元和输出分别表达为：

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-6c8dd146d7ad3ff89d8f7e377d718c7e_hd.jpg)

为了计算简便一些，我们需要将其向量化。

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-81e780d8d0d5ebb4e488ee6cb057bc9b_hd.jpg)

神经网络的权重其实本质上也就是之前多项式的系数，外加的偏置单元也就是常数项。我们可以把a0 ,a1 ,a2 ,a3 看成更为高级的特征值。

![1564056367601](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564056367601.png)

##### **神经网络代价函数**

来观察算法预测的结果与真实情况的误差有多大

![1564056501562](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564056501562.png)

##### **反向传播算法**

目的是更新权值，重新计算输出。首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。

查看https://www.cnblogs.com/charlotte77/p/5629865.html和https://www.zybuluo.com/hanbingtao/note/476663了解详情

接受 dL/dz，即相对于 z 的损失函数的梯度，损失函数中 x 和 y 的梯度可以通过应用链式法则进行计算，如下图所示。

![img](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-c96e284f4ab319a02fdd762367b58774_hd.jpg)

![1564056750829](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564056750829.png)

![1564057897991](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564057897991.png)

##### **梯度的数值检验（Numerical Gradient Checking）**

通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。

##### **参数随机初始化**

任何优化算法都需要一些初始的参数，初始参数为正负𝜁之间的随机值

#### 机器学习技巧——训练集，开发集，测试集

training set：训练集是用来训练模型的。遵循训练集大，开发，测试集小的特点，占了所有数据的绝大部分。

development set：用来对训练集训练出来的模型进行测试，通过测试结果来不断地优化模型。

test set：在训练结束后，对训练出的模型进行一次最终的评估所用的数据集。

```
1. 获得更多的训练实例——解决高方差
2. 尝试减少特征的数量——解决高方差
3. 尝试获得更多的特征——解决高偏差
4. 尝试增加多项式特征——解决高偏差
5. 尝试减少正则化程度 λ——解决高偏差
6. 尝试增加正则化程度 λ——解决高方差
```

使用 60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用 20%的数据作为测试集，在交叉验证集上来做误差分析。

```
1. 使用训练集训练出 10 个模型
2. 用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）
3. 选取代价函数值最小的模型
4. 用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）
```

##### **偏差和方差**

![1564059326314](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564059326314.png)

##### **选择λ的方法为：**

```
1.使用训练集训练出 12 个不同程度正则化的模型
2.用 12 个模型分别对交叉验证集计算的出交叉验证误差
3.选择得出交叉验证误差 最小的模型
4.运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与 λ 的值绘制在一张图表上：
```

![1564059593223](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564059593223.png)

通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。
对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。

##### **构建一个学习算法的推荐方法为**

```
1. 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法
2. 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择
3. 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势
```

真正能提高性能的，是你能够给一个算法大量的训练数据

##### **查准率（Precision）和 查全率（Recall）** 

我们将算法预测的结果分成四种情况：

1.  正确肯定（True Positive,TP）：预测为真，实际为真
2.  正确否定（True Negative,TN）：预测为假，实际为假
3.  错误肯定（False Positive,FP）：预测为真，实际为假
4.  错误否定（False Negative,FN）：预测为假，实际为真

则：查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。
查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。

#####  **F1  值（F1 Score）**

其计算公式为：F1=2*(PR)/(P+R)

#### 支持向量机

##### 支持向量机(Support Vector Machine)

它努力用一个最大间距来分离样本。因此支持向量机有时被称为 大间距分类器

![1564105421766](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564105421766.png)

如果你有一个正样本，我们会希望z>=1，反之，如果y= 0，我们观察一下，函数cost0 (z)，它只有在z<= −1的区间里函数值为 0

回顾 C = 1/ λ，因此：
C较大时，相当于 λ较小，可能会导致过拟合，高方差。
C较小时，相当于 λ较大，可能会导致低拟合，高偏差

实例：最大化到分类线的距离

![1564107819811](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564107819811.png)



##### 核函数

计算新的特征，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征f1 ,f2 ,f3 。

![1564108220089](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564108220089.png)

𝜎较大时，可能会导致低方差，高偏差；

𝜎较小时，可能会导致低偏差，高方差



##### 逻辑回归模型or支持向量机模型

```
𝑜为特征数，𝑛为训练样本数。
(1)如果相较于𝑛而言，𝑜要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。
(2)如果𝑜较小，而且𝑛大小中等，例如𝑜在 1-1000 之间，而𝑛在 10-10000 之间，使用高斯核函数的支持向量机。
(3)如果𝑜较小，而𝑛较大，例如𝑜在 1-1000 之间，而𝑛大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机
```

#### 聚类

##### **K-Means Algorithm** 

最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。算法分为两个步骤，第一个 for 循环是赋值步骤，即：对于每一个样例i，计算其应该属于的类。第二个 for 循环是聚类中心的移动，即：对于每一个类K，重新计算该类的质心。

K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称 畸变函数 Distortion function）为：

![1564130487945](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564130487945.png)

在运行 K- 均值算法的之前，我们首先要**随机初始化**所有的聚类中心点，没有所谓最好的**选择聚类数**的方法，通常是需要根据不同的问题，人工进行选择的。

##### 聚类的衡量指标

(1). 均一性：p
类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个聚簇中正确分类的样本数占该聚簇总样本数的比例和)
(2). 完整性：r
类似于召回率，同类别样本被归类到相同簇中，则满足完整性;(每个聚簇中正确分类的样本数占该类型的总样本数比例的和)

(3). V-measure: 均一性和完整性的加权平均V =(1 + β * β )* p r/((β * β )*p+r)

##### 降维

把任何维度的数据降到任何想要的维度，用于数据压缩和数据可视化。

##### 主成分分析（Principal Component Analysis，PCA）

在 PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。投射误差是从特征向量向该方向向量作垂线的长度,完全无参数限制,计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关.

![1564132382657](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1564132382657.png)

一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。其次，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。

##### 两个实际案例

异常检测和推荐系统

#### 大规模机器学习

Stochastic Gradient Descent**随机梯度下降法（SGD）**代替批量梯度下降法。

![1566206665379](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1566206665379.png)

**小批量梯度下降算法**是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数𝑐次训练实例，便更新一次参数 θ。

**在线学习**

一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。

旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。

**映射化简和数据并行**（Map Reduce and Data Parallelism）对于大规模机器学习问题而言是非常重要的概念。

如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。

##### 应用实例：图片文字识别

1.文字侦测（Text detection）——将图片上的文字与其他环境对象分离开来
2.字符切分（Character segmentation）——将文字分割成一个个单一的字符
3.字符分类（Character classification）——确定每一个字符是什么 可以用任务流程图来

**滑动窗口**是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。

**有关获得更多数据的几种方法：**
1.人工数据合成
2.手动收集、标记数据
3.众包

**上限分析**

在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。

#### 课程总结

![1566207481351](https://cdn.jsdelivr.net/gh/siyuanzhou/pic@master/pic/2019-07-24-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1566207481351.png)

#### 机器学习数学基础知识

{% pdf ../pdf/数学基础.pdf %}

